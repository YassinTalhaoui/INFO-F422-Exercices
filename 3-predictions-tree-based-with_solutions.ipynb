{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO-F-422 -  Statistical Foundations of Machine Learning \n",
    "\n",
    "### Jacopo De Stefani - __[Jacopo.De.Stefani@ulb.ac.be](mailto:Jacopo.De.Stefani@ulb.ac.be)__\n",
    "### Th√©o Verhelst - __[Theo.Verhelst@ulb.ac.be](mailto:Theo.Verhelst@ulb.ac.be)__\n",
    "### Gianluca Bontempi - __[gbonte@ulb.ac.be](mailto:gbonte@ulb.ac.be)__\n",
    "\n",
    "## TP 3 - Predictions: Tree-based methods\n",
    "\n",
    "####  March 23, 2021\n",
    "\n",
    "#### Materials originally developed by *Bertrand Lebichot, Jacopo De Stefani and Gianluca Bontempi*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "### Supervised learning\n",
    "\n",
    "The process of supervised learning involves the presence of an entity (the learner, also called prediction model), whose goal is to learn the mapping between inputs and outputs in a given problem.\n",
    "\n",
    "A supervised learning problem can formulated as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    " y = m(\\mathbf{x})  \n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    "- $y$ represents the output variable (also called target)\n",
    "- $\\mathbf{x}$ represents the vector of inputs (also called features).\n",
    "- $m$ is the (unknown) mapping between input and outputs.\n",
    "\n",
    "In the majority of the supervised learning problems, the mapping $m$ between input and outputs is unknown and needs to be estimated on basis of the available input/output observation pairs $(\\mathbf{x}_i,y_i)$.\n",
    "\n",
    "## Classification vs regression\n",
    "\n",
    "Both classification and regression are sub-fields of *supervised learning*. In the two cases, we have predictive variables $\\mathbf{x}$ and a target variable $y$. \n",
    "The main difference betweet the two type of problems is the type of the target variabile:\n",
    "\n",
    "- In classification, $y$ is a discrete variable; i.e $y \\in \\{C_1,\\cdots,C_k\\}$\n",
    "- In regression, $y$ is a continuous variable; i.e $y \\in \\mathbb{R}$\n",
    "\n",
    "In this practical, in order to better understand the classification process, we will tackle the simplest classification case, with $k=2$ possible output classes (called binary classification).\n",
    "\n",
    "The goal of our approach is to be able to learn the mapping between the input features and the predefined discrete output classes, in order to be able to perform an automatic classification, based on the available data. \n",
    "\n",
    "Let's illustrate the problem with a simple example :\n",
    "* Knowing the two abstract features $X_1$ and $X_2$, and $n=6$ samples for both class (the red class and the blue class), can you predict the label for the green sample ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAAP8A/wBNTU1o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\nrkJggg==",
      "text/plain": [
       "Plot with title \"Scatterplot and decision boundary\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#options(repr.plot.width=20, repr.plot.height=10)\n",
    "n = 6\n",
    "\n",
    "# blue class data\n",
    "X1 = rnorm(n, mean = +2, sd = 1)\n",
    "X2 = rnorm(n, mean = +1, sd = 1)\n",
    "SamplesBLUE = data.frame(X1, X2)\n",
    "gBLUE = c(mean(X1),mean(X2))\n",
    "\n",
    "# red class data\n",
    "X1 = rnorm(n, mean = -2, sd = 1)\n",
    "X2 = rnorm(n, mean = -1, sd = 1)\n",
    "SamplesRED = data.frame(X1, X2)\n",
    "gRED = c(mean(X1),mean(X2))\n",
    "\n",
    "# draw scatterplot\n",
    "plot(SamplesBLUE$X1,SamplesBLUE$X2,col=\"blue\",main=\"Scatterplot and decision boundary\",xlab=\"X1\",ylab=\"X2\",xlim=c(-5,5),ylim=c(-5,5))\n",
    "points(SamplesRED$X1,SamplesRED$X2,col=\"red\")\n",
    "points(0,0,col=\"green\")\n",
    "\n",
    "# draw centroid of classes\n",
    "points(gBLUE[1],gBLUE[2],col=\"blue\",pch = 4)\n",
    "points(gRED[1],gRED[2],col=\"red\",pch = 4)\n",
    "\n",
    "# draw decision boundary\n",
    "midgg = c(gRED[1]+abs(gBLUE[1]-gRED[1])/2,gRED[2]+abs(gBLUE[2]-gRED[2])/2)\n",
    "slope = -(gBLUE[1]-gRED[1])/(gBLUE[2]-gRED[2])\n",
    "abline(a=midgg[2]-slope*midgg[1],b=slope)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification metrics\n",
    "\n",
    "### Confusion Matrix \n",
    "\n",
    "A confusion matrix $\\mathbf{C}$ is a $k \\times k$ matrix containing the classifications statistics of a given classifier.\n",
    "$c_{ij}$ containes the number of times that a sample belonging to the actual class $j$ has been predicted as belonging to class $i$.\n",
    "\n",
    "In our two-class setting, the confusion matrix (reporting the number of actual class / predicted class) has four entries:\n",
    " \n",
    "|            | Actual Negative   | Actual Positive  |\n",
    "|:----------:|-------------------|-----------------|\n",
    "|**Classified Negative** | $T_N$ (True Negative) | $F_N$ (False Negative)| \n",
    "|**Classified Positive** | $F_P$ (False Positive) | $T_P$ (True Positive) |\n",
    "\n",
    "Ideally, if the classifier didn't make any mistake, the confusion matrix $\\mathbf{C}$ should be diagonal.\n",
    "By looking at the off-diagonal elements, we can understand which kind of mistakes the classifier is making (e.g. Actual Negative -> Predicted Positive, Actual Positive -> Predicted Negative).\n",
    "\n",
    "Additionally, the confusion matrix allows to compute the total number of elements classified negative $\\hat{N_N}$, classified positive $\\hat{N_P}$, actual negative $N_N$, actual positive $N_P$, as well as the total number of samples $N$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{N_N} = T_N + F_N & & & \\hat{N_P} = T_P + F_P \\\\ \n",
    "N_N = T_N + F_P & & & N_P = T_P + F_N \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation}\n",
    "N = T_N + F_N + T_P + F_P \n",
    "\\end{equation}\n",
    "\n",
    "The quantities in the confusion matrix are used to define different accuracy measures, such as:\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "\\begin{equation}\n",
    "A = \\frac{T_P + T_N}{N} = \\frac{T_P + T_N}{F_P + F_N + T_P + T_N} \n",
    "\\end{equation}\n",
    "\n",
    "The accuracy represents the ratio between the number of correctly classified samples (False ...) and the total number of samples.\n",
    "\n",
    "#### Misclassification Rate\n",
    "\n",
    "\\begin{equation}\n",
    "ER = \\frac{F_P + F_N}{N} = \\frac{F_P + F_N}{F_P + F_N + T_P + T_N} \n",
    "\\end{equation}\n",
    "\n",
    "The misclassification rate represents the number of total classification mistakes (False ...) over the total number of samples.\n",
    "It can be shown that ER=1-A.\n",
    "\n",
    "#### Balanced Error Rate\n",
    "\\begin{equation}\n",
    "BER = \\frac{1}{2}(\\frac{F_P}{N_N} + \\frac{F_N}{N_P}) = \\frac{1}{2}(\\frac{F_P}{T_N + F_P} + \\frac{F_N}{T_P + F_N}) \n",
    "\\end{equation}\n",
    "\n",
    "The balanced error rate is computed as an average of the error for each classes.\n",
    "For unbalanced classification problems (i.e. $N_N \\neq N_P$ the total number of samples belonging to the negative and positive classes is different).\n",
    "\n",
    "#### Sensitivity and specificity\n",
    "\\begin{align}\n",
    "SE = \\frac{T_P}{N_P} = \\frac{T_P}{T_P + F_N} & & & 0 \\leq SE \\leq 1 \n",
    "\\end{align}\n",
    "\n",
    "The sensitivity (also called recall) is a measure defined as the ratio between the correctly classified positive samples over the total number of positive samples (i.e True Positive rate). It measures the impact of false negatives on the classification process.\n",
    "\n",
    "\\begin{align}\n",
    "SP = \\frac{T_N}{N_N} = \\frac{T_N}{F_P + T_N}  & & & 0 \\leq SP \\leq 1\n",
    "\\end{align}\n",
    "\n",
    "The specificity is a measure defined as the ratio between the correctly classified negative samples over the total number of negative samples (i.e True Negative rate). It measures the impact of false positive on the classification process.\n",
    "\n",
    "The need for different accuracy measures arises from the fact that the impact of a certain type of error (e.g. Actual Negative -> Predicted Positive, Actual Positive -> Predicted Negative) might be greatly different, according to the context in which the classification problem is performed (for example medical diagnosis, fraud detection).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The chosen database is about spam detection. Details can be found here :\n",
    "https://archive.ics.uci.edu/ml/datasets/spambase\n",
    "\n",
    "To import the dataset, just use package ''kernlab''\n",
    "\n",
    "`install.packages(\"kernlab\")`\n",
    "\n",
    "In case of doubts, always remember that R can provide you with the documentation of a function package/using the following syntax:\n",
    "`? name_of_function` or `help(name_of_function)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'tree' was built under R version 4.0.5\"\n",
      "Warning message:\n",
      "\"package 'rpart' was built under R version 4.0.4\"\n"
     ]
    }
   ],
   "source": [
    "library(\"kernlab\")\n",
    "library(\"tree\")\n",
    "library(\"rpart\")\n",
    "data(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 0 - Exploratory Data Anaylsis \n",
    "\n",
    "You first have to explore the dataset before going to classification. This step can be long but is actually very important. In particular, obtain or observe the following interesting elements :\n",
    "* Number of rows and columns\n",
    "* Is there any missing values ?\n",
    "* Spam prior\n",
    "* Name and basic statistics for each variables\n",
    "* Histogram (or other relevant plot) per class (spam vs non-spam)\n",
    "\n",
    "### Number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>4601</li><li>58</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4601\n",
       "\\item 58\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4601\n",
       "2. 58\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 4601   58"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presence of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>4601</li><li>58</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4601\n",
       "\\item 58\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4601\n",
       "2. 58\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 4601   58"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# No missing values -> dim(na.omit(dataset)) == dim(dataset)\n",
    "dim(na.omit(spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>make</dt><dd>FALSE</dd><dt>address</dt><dd>FALSE</dd><dt>all</dt><dd>FALSE</dd><dt>num3d</dt><dd>FALSE</dd><dt>our</dt><dd>FALSE</dd><dt>over</dt><dd>FALSE</dd><dt>remove</dt><dd>FALSE</dd><dt>internet</dt><dd>FALSE</dd><dt>order</dt><dd>FALSE</dd><dt>mail</dt><dd>FALSE</dd><dt>receive</dt><dd>FALSE</dd><dt>will</dt><dd>FALSE</dd><dt>people</dt><dd>FALSE</dd><dt>report</dt><dd>FALSE</dd><dt>addresses</dt><dd>FALSE</dd><dt>free</dt><dd>FALSE</dd><dt>business</dt><dd>FALSE</dd><dt>email</dt><dd>FALSE</dd><dt>you</dt><dd>FALSE</dd><dt>credit</dt><dd>FALSE</dd><dt>your</dt><dd>FALSE</dd><dt>font</dt><dd>FALSE</dd><dt>num000</dt><dd>FALSE</dd><dt>money</dt><dd>FALSE</dd><dt>hp</dt><dd>FALSE</dd><dt>hpl</dt><dd>FALSE</dd><dt>george</dt><dd>FALSE</dd><dt>num650</dt><dd>FALSE</dd><dt>lab</dt><dd>FALSE</dd><dt>labs</dt><dd>FALSE</dd><dt>telnet</dt><dd>FALSE</dd><dt>num857</dt><dd>FALSE</dd><dt>data</dt><dd>FALSE</dd><dt>num415</dt><dd>FALSE</dd><dt>num85</dt><dd>FALSE</dd><dt>technology</dt><dd>FALSE</dd><dt>num1999</dt><dd>FALSE</dd><dt>parts</dt><dd>FALSE</dd><dt>pm</dt><dd>FALSE</dd><dt>direct</dt><dd>FALSE</dd><dt>cs</dt><dd>FALSE</dd><dt>meeting</dt><dd>FALSE</dd><dt>original</dt><dd>FALSE</dd><dt>project</dt><dd>FALSE</dd><dt>re</dt><dd>FALSE</dd><dt>edu</dt><dd>FALSE</dd><dt>table</dt><dd>FALSE</dd><dt>conference</dt><dd>FALSE</dd><dt>charSemicolon</dt><dd>FALSE</dd><dt>charRoundbracket</dt><dd>FALSE</dd><dt>charSquarebracket</dt><dd>FALSE</dd><dt>charExclamation</dt><dd>FALSE</dd><dt>charDollar</dt><dd>FALSE</dd><dt>charHash</dt><dd>FALSE</dd><dt>capitalAve</dt><dd>FALSE</dd><dt>capitalLong</dt><dd>FALSE</dd><dt>capitalTotal</dt><dd>FALSE</dd><dt>type</dt><dd>FALSE</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[make] FALSE\n",
       "\\item[address] FALSE\n",
       "\\item[all] FALSE\n",
       "\\item[num3d] FALSE\n",
       "\\item[our] FALSE\n",
       "\\item[over] FALSE\n",
       "\\item[remove] FALSE\n",
       "\\item[internet] FALSE\n",
       "\\item[order] FALSE\n",
       "\\item[mail] FALSE\n",
       "\\item[receive] FALSE\n",
       "\\item[will] FALSE\n",
       "\\item[people] FALSE\n",
       "\\item[report] FALSE\n",
       "\\item[addresses] FALSE\n",
       "\\item[free] FALSE\n",
       "\\item[business] FALSE\n",
       "\\item[email] FALSE\n",
       "\\item[you] FALSE\n",
       "\\item[credit] FALSE\n",
       "\\item[your] FALSE\n",
       "\\item[font] FALSE\n",
       "\\item[num000] FALSE\n",
       "\\item[money] FALSE\n",
       "\\item[hp] FALSE\n",
       "\\item[hpl] FALSE\n",
       "\\item[george] FALSE\n",
       "\\item[num650] FALSE\n",
       "\\item[lab] FALSE\n",
       "\\item[labs] FALSE\n",
       "\\item[telnet] FALSE\n",
       "\\item[num857] FALSE\n",
       "\\item[data] FALSE\n",
       "\\item[num415] FALSE\n",
       "\\item[num85] FALSE\n",
       "\\item[technology] FALSE\n",
       "\\item[num1999] FALSE\n",
       "\\item[parts] FALSE\n",
       "\\item[pm] FALSE\n",
       "\\item[direct] FALSE\n",
       "\\item[cs] FALSE\n",
       "\\item[meeting] FALSE\n",
       "\\item[original] FALSE\n",
       "\\item[project] FALSE\n",
       "\\item[re] FALSE\n",
       "\\item[edu] FALSE\n",
       "\\item[table] FALSE\n",
       "\\item[conference] FALSE\n",
       "\\item[charSemicolon] FALSE\n",
       "\\item[charRoundbracket] FALSE\n",
       "\\item[charSquarebracket] FALSE\n",
       "\\item[charExclamation] FALSE\n",
       "\\item[charDollar] FALSE\n",
       "\\item[charHash] FALSE\n",
       "\\item[capitalAve] FALSE\n",
       "\\item[capitalLong] FALSE\n",
       "\\item[capitalTotal] FALSE\n",
       "\\item[type] FALSE\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "make\n",
       ":   FALSEaddress\n",
       ":   FALSEall\n",
       ":   FALSEnum3d\n",
       ":   FALSEour\n",
       ":   FALSEover\n",
       ":   FALSEremove\n",
       ":   FALSEinternet\n",
       ":   FALSEorder\n",
       ":   FALSEmail\n",
       ":   FALSEreceive\n",
       ":   FALSEwill\n",
       ":   FALSEpeople\n",
       ":   FALSEreport\n",
       ":   FALSEaddresses\n",
       ":   FALSEfree\n",
       ":   FALSEbusiness\n",
       ":   FALSEemail\n",
       ":   FALSEyou\n",
       ":   FALSEcredit\n",
       ":   FALSEyour\n",
       ":   FALSEfont\n",
       ":   FALSEnum000\n",
       ":   FALSEmoney\n",
       ":   FALSEhp\n",
       ":   FALSEhpl\n",
       ":   FALSEgeorge\n",
       ":   FALSEnum650\n",
       ":   FALSElab\n",
       ":   FALSElabs\n",
       ":   FALSEtelnet\n",
       ":   FALSEnum857\n",
       ":   FALSEdata\n",
       ":   FALSEnum415\n",
       ":   FALSEnum85\n",
       ":   FALSEtechnology\n",
       ":   FALSEnum1999\n",
       ":   FALSEparts\n",
       ":   FALSEpm\n",
       ":   FALSEdirect\n",
       ":   FALSEcs\n",
       ":   FALSEmeeting\n",
       ":   FALSEoriginal\n",
       ":   FALSEproject\n",
       ":   FALSEre\n",
       ":   FALSEedu\n",
       ":   FALSEtable\n",
       ":   FALSEconference\n",
       ":   FALSEcharSemicolon\n",
       ":   FALSEcharRoundbracket\n",
       ":   FALSEcharSquarebracket\n",
       ":   FALSEcharExclamation\n",
       ":   FALSEcharDollar\n",
       ":   FALSEcharHash\n",
       ":   FALSEcapitalAve\n",
       ":   FALSEcapitalLong\n",
       ":   FALSEcapitalTotal\n",
       ":   FALSEtype\n",
       ":   FALSE\n",
       "\n"
      ],
      "text/plain": [
       "             make           address               all             num3d \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "              our              over            remove          internet \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "            order              mail           receive              will \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "           people            report         addresses              free \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "         business             email               you            credit \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "             your              font            num000             money \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "               hp               hpl            george            num650 \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "              lab              labs            telnet            num857 \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "             data            num415             num85        technology \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "          num1999             parts                pm            direct \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "               cs           meeting          original           project \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "               re               edu             table        conference \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "    charSemicolon  charRoundbracket charSquarebracket   charExclamation \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "       charDollar          charHash        capitalAve       capitalLong \n",
       "            FALSE             FALSE             FALSE             FALSE \n",
       "     capitalTotal              type \n",
       "            FALSE             FALSE "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2nd Check\n",
    "apply(is.na(spam),2,any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 √ó 58</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>make</th><th scope=col>address</th><th scope=col>all</th><th scope=col>num3d</th><th scope=col>our</th><th scope=col>over</th><th scope=col>remove</th><th scope=col>internet</th><th scope=col>order</th><th scope=col>mail</th><th scope=col>...</th><th scope=col>charSemicolon</th><th scope=col>charRoundbracket</th><th scope=col>charSquarebracket</th><th scope=col>charExclamation</th><th scope=col>charDollar</th><th scope=col>charHash</th><th scope=col>capitalAve</th><th scope=col>capitalLong</th><th scope=col>capitalTotal</th><th scope=col>type</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>...</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0.00</td><td>0.64</td><td>0.64</td><td>0</td><td>0.32</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>...</td><td>0.00</td><td>0.000</td><td>0</td><td>0.778</td><td>0.000</td><td>0.000</td><td>3.756</td><td> 61</td><td> 278</td><td>spam</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>0.21</td><td>0.28</td><td>0.50</td><td>0</td><td>0.14</td><td>0.28</td><td>0.21</td><td>0.07</td><td>0.00</td><td>0.94</td><td>...</td><td>0.00</td><td>0.132</td><td>0</td><td>0.372</td><td>0.180</td><td>0.048</td><td>5.114</td><td>101</td><td>1028</td><td>spam</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0.06</td><td>0.00</td><td>0.71</td><td>0</td><td>1.23</td><td>0.19</td><td>0.19</td><td>0.12</td><td>0.64</td><td>0.25</td><td>...</td><td>0.01</td><td>0.143</td><td>0</td><td>0.276</td><td>0.184</td><td>0.010</td><td>9.821</td><td>485</td><td>2259</td><td>spam</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0.00</td><td>0.00</td><td>0.00</td><td>0</td><td>0.63</td><td>0.00</td><td>0.31</td><td>0.63</td><td>0.31</td><td>0.63</td><td>...</td><td>0.00</td><td>0.137</td><td>0</td><td>0.137</td><td>0.000</td><td>0.000</td><td>3.537</td><td> 40</td><td> 191</td><td>spam</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0.00</td><td>0.00</td><td>0.00</td><td>0</td><td>0.63</td><td>0.00</td><td>0.31</td><td>0.63</td><td>0.31</td><td>0.63</td><td>...</td><td>0.00</td><td>0.135</td><td>0</td><td>0.135</td><td>0.000</td><td>0.000</td><td>3.537</td><td> 40</td><td> 191</td><td>spam</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>0.00</td><td>0.00</td><td>0.00</td><td>0</td><td>1.85</td><td>0.00</td><td>0.00</td><td>1.85</td><td>0.00</td><td>0.00</td><td>...</td><td>0.00</td><td>0.223</td><td>0</td><td>0.000</td><td>0.000</td><td>0.000</td><td>3.000</td><td> 15</td><td>  54</td><td>spam</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 √ó 58\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & make & address & all & num3d & our & over & remove & internet & order & mail & ... & charSemicolon & charRoundbracket & charSquarebracket & charExclamation & charDollar & charHash & capitalAve & capitalLong & capitalTotal & type\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ... & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t1 & 0.00 & 0.64 & 0.64 & 0 & 0.32 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & ... & 0.00 & 0.000 & 0 & 0.778 & 0.000 & 0.000 & 3.756 &  61 &  278 & spam\\\\\n",
       "\t2 & 0.21 & 0.28 & 0.50 & 0 & 0.14 & 0.28 & 0.21 & 0.07 & 0.00 & 0.94 & ... & 0.00 & 0.132 & 0 & 0.372 & 0.180 & 0.048 & 5.114 & 101 & 1028 & spam\\\\\n",
       "\t3 & 0.06 & 0.00 & 0.71 & 0 & 1.23 & 0.19 & 0.19 & 0.12 & 0.64 & 0.25 & ... & 0.01 & 0.143 & 0 & 0.276 & 0.184 & 0.010 & 9.821 & 485 & 2259 & spam\\\\\n",
       "\t4 & 0.00 & 0.00 & 0.00 & 0 & 0.63 & 0.00 & 0.31 & 0.63 & 0.31 & 0.63 & ... & 0.00 & 0.137 & 0 & 0.137 & 0.000 & 0.000 & 3.537 &  40 &  191 & spam\\\\\n",
       "\t5 & 0.00 & 0.00 & 0.00 & 0 & 0.63 & 0.00 & 0.31 & 0.63 & 0.31 & 0.63 & ... & 0.00 & 0.135 & 0 & 0.135 & 0.000 & 0.000 & 3.537 &  40 &  191 & spam\\\\\n",
       "\t6 & 0.00 & 0.00 & 0.00 & 0 & 1.85 & 0.00 & 0.00 & 1.85 & 0.00 & 0.00 & ... & 0.00 & 0.223 & 0 & 0.000 & 0.000 & 0.000 & 3.000 &  15 &   54 & spam\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 √ó 58\n",
       "\n",
       "| <!--/--> | make &lt;dbl&gt; | address &lt;dbl&gt; | all &lt;dbl&gt; | num3d &lt;dbl&gt; | our &lt;dbl&gt; | over &lt;dbl&gt; | remove &lt;dbl&gt; | internet &lt;dbl&gt; | order &lt;dbl&gt; | mail &lt;dbl&gt; | ... ... | charSemicolon &lt;dbl&gt; | charRoundbracket &lt;dbl&gt; | charSquarebracket &lt;dbl&gt; | charExclamation &lt;dbl&gt; | charDollar &lt;dbl&gt; | charHash &lt;dbl&gt; | capitalAve &lt;dbl&gt; | capitalLong &lt;dbl&gt; | capitalTotal &lt;dbl&gt; | type &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 0.00 | 0.64 | 0.64 | 0 | 0.32 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | ... | 0.00 | 0.000 | 0 | 0.778 | 0.000 | 0.000 | 3.756 |  61 |  278 | spam |\n",
       "| 2 | 0.21 | 0.28 | 0.50 | 0 | 0.14 | 0.28 | 0.21 | 0.07 | 0.00 | 0.94 | ... | 0.00 | 0.132 | 0 | 0.372 | 0.180 | 0.048 | 5.114 | 101 | 1028 | spam |\n",
       "| 3 | 0.06 | 0.00 | 0.71 | 0 | 1.23 | 0.19 | 0.19 | 0.12 | 0.64 | 0.25 | ... | 0.01 | 0.143 | 0 | 0.276 | 0.184 | 0.010 | 9.821 | 485 | 2259 | spam |\n",
       "| 4 | 0.00 | 0.00 | 0.00 | 0 | 0.63 | 0.00 | 0.31 | 0.63 | 0.31 | 0.63 | ... | 0.00 | 0.137 | 0 | 0.137 | 0.000 | 0.000 | 3.537 |  40 |  191 | spam |\n",
       "| 5 | 0.00 | 0.00 | 0.00 | 0 | 0.63 | 0.00 | 0.31 | 0.63 | 0.31 | 0.63 | ... | 0.00 | 0.135 | 0 | 0.135 | 0.000 | 0.000 | 3.537 |  40 |  191 | spam |\n",
       "| 6 | 0.00 | 0.00 | 0.00 | 0 | 1.85 | 0.00 | 0.00 | 1.85 | 0.00 | 0.00 | ... | 0.00 | 0.223 | 0 | 0.000 | 0.000 | 0.000 | 3.000 |  15 |   54 | spam |\n",
       "\n"
      ],
      "text/plain": [
       "  make address all  num3d our  over remove internet order mail ...\n",
       "1 0.00 0.64    0.64 0     0.32 0.00 0.00   0.00     0.00  0.00 ...\n",
       "2 0.21 0.28    0.50 0     0.14 0.28 0.21   0.07     0.00  0.94 ...\n",
       "3 0.06 0.00    0.71 0     1.23 0.19 0.19   0.12     0.64  0.25 ...\n",
       "4 0.00 0.00    0.00 0     0.63 0.00 0.31   0.63     0.31  0.63 ...\n",
       "5 0.00 0.00    0.00 0     0.63 0.00 0.31   0.63     0.31  0.63 ...\n",
       "6 0.00 0.00    0.00 0     1.85 0.00 0.00   1.85     0.00  0.00 ...\n",
       "  charSemicolon charRoundbracket charSquarebracket charExclamation charDollar\n",
       "1 0.00          0.000            0                 0.778           0.000     \n",
       "2 0.00          0.132            0                 0.372           0.180     \n",
       "3 0.01          0.143            0                 0.276           0.184     \n",
       "4 0.00          0.137            0                 0.137           0.000     \n",
       "5 0.00          0.135            0                 0.135           0.000     \n",
       "6 0.00          0.223            0                 0.000           0.000     \n",
       "  charHash capitalAve capitalLong capitalTotal type\n",
       "1 0.000    3.756       61          278         spam\n",
       "2 0.048    5.114      101         1028         spam\n",
       "3 0.010    9.821      485         2259         spam\n",
       "4 0.000    3.537       40          191         spam\n",
       "5 0.000    3.537       40          191         spam\n",
       "6 0.000    3.000       15           54         spam"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      make           address            all             num3d         \n",
       " Min.   :0.0000   Min.   : 0.000   Min.   :0.0000   Min.   : 0.00000  \n",
       " 1st Qu.:0.0000   1st Qu.: 0.000   1st Qu.:0.0000   1st Qu.: 0.00000  \n",
       " Median :0.0000   Median : 0.000   Median :0.0000   Median : 0.00000  \n",
       " Mean   :0.1046   Mean   : 0.213   Mean   :0.2807   Mean   : 0.06542  \n",
       " 3rd Qu.:0.0000   3rd Qu.: 0.000   3rd Qu.:0.4200   3rd Qu.: 0.00000  \n",
       " Max.   :4.5400   Max.   :14.280   Max.   :5.1000   Max.   :42.81000  \n",
       "      our               over            remove          internet      \n",
       " Min.   : 0.0000   Min.   :0.0000   Min.   :0.0000   Min.   : 0.0000  \n",
       " 1st Qu.: 0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 0.0000  \n",
       " Median : 0.0000   Median :0.0000   Median :0.0000   Median : 0.0000  \n",
       " Mean   : 0.3122   Mean   :0.0959   Mean   :0.1142   Mean   : 0.1053  \n",
       " 3rd Qu.: 0.3800   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.: 0.0000  \n",
       " Max.   :10.0000   Max.   :5.8800   Max.   :7.2700   Max.   :11.1100  \n",
       "     order              mail            receive             will       \n",
       " Min.   :0.00000   Min.   : 0.0000   Min.   :0.00000   Min.   :0.0000  \n",
       " 1st Qu.:0.00000   1st Qu.: 0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n",
       " Median :0.00000   Median : 0.0000   Median :0.00000   Median :0.1000  \n",
       " Mean   :0.09007   Mean   : 0.2394   Mean   :0.05982   Mean   :0.5417  \n",
       " 3rd Qu.:0.00000   3rd Qu.: 0.1600   3rd Qu.:0.00000   3rd Qu.:0.8000  \n",
       " Max.   :5.26000   Max.   :18.1800   Max.   :2.61000   Max.   :9.6700  \n",
       "     people            report           addresses           free        \n",
       " Min.   :0.00000   Min.   : 0.00000   Min.   :0.0000   Min.   : 0.0000  \n",
       " 1st Qu.:0.00000   1st Qu.: 0.00000   1st Qu.:0.0000   1st Qu.: 0.0000  \n",
       " Median :0.00000   Median : 0.00000   Median :0.0000   Median : 0.0000  \n",
       " Mean   :0.09393   Mean   : 0.05863   Mean   :0.0492   Mean   : 0.2488  \n",
       " 3rd Qu.:0.00000   3rd Qu.: 0.00000   3rd Qu.:0.0000   3rd Qu.: 0.1000  \n",
       " Max.   :5.55000   Max.   :10.00000   Max.   :4.4100   Max.   :20.0000  \n",
       "    business          email             you             credit        \n",
       " Min.   :0.0000   Min.   :0.0000   Min.   : 0.000   Min.   : 0.00000  \n",
       " 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 0.000   1st Qu.: 0.00000  \n",
       " Median :0.0000   Median :0.0000   Median : 1.310   Median : 0.00000  \n",
       " Mean   :0.1426   Mean   :0.1847   Mean   : 1.662   Mean   : 0.08558  \n",
       " 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.: 2.640   3rd Qu.: 0.00000  \n",
       " Max.   :7.1400   Max.   :9.0900   Max.   :18.750   Max.   :18.18000  \n",
       "      your              font             num000           money         \n",
       " Min.   : 0.0000   Min.   : 0.0000   Min.   :0.0000   Min.   : 0.00000  \n",
       " 1st Qu.: 0.0000   1st Qu.: 0.0000   1st Qu.:0.0000   1st Qu.: 0.00000  \n",
       " Median : 0.2200   Median : 0.0000   Median :0.0000   Median : 0.00000  \n",
       " Mean   : 0.8098   Mean   : 0.1212   Mean   :0.1016   Mean   : 0.09427  \n",
       " 3rd Qu.: 1.2700   3rd Qu.: 0.0000   3rd Qu.:0.0000   3rd Qu.: 0.00000  \n",
       " Max.   :11.1100   Max.   :17.1000   Max.   :5.4500   Max.   :12.50000  \n",
       "       hp               hpl              george            num650      \n",
       " Min.   : 0.0000   Min.   : 0.0000   Min.   : 0.0000   Min.   :0.0000  \n",
       " 1st Qu.: 0.0000   1st Qu.: 0.0000   1st Qu.: 0.0000   1st Qu.:0.0000  \n",
       " Median : 0.0000   Median : 0.0000   Median : 0.0000   Median :0.0000  \n",
       " Mean   : 0.5495   Mean   : 0.2654   Mean   : 0.7673   Mean   :0.1248  \n",
       " 3rd Qu.: 0.0000   3rd Qu.: 0.0000   3rd Qu.: 0.0000   3rd Qu.:0.0000  \n",
       " Max.   :20.8300   Max.   :16.6600   Max.   :33.3300   Max.   :9.0900  \n",
       "      lab                labs            telnet             num857       \n",
       " Min.   : 0.00000   Min.   :0.0000   Min.   : 0.00000   Min.   :0.00000  \n",
       " 1st Qu.: 0.00000   1st Qu.:0.0000   1st Qu.: 0.00000   1st Qu.:0.00000  \n",
       " Median : 0.00000   Median :0.0000   Median : 0.00000   Median :0.00000  \n",
       " Mean   : 0.09892   Mean   :0.1029   Mean   : 0.06475   Mean   :0.04705  \n",
       " 3rd Qu.: 0.00000   3rd Qu.:0.0000   3rd Qu.: 0.00000   3rd Qu.:0.00000  \n",
       " Max.   :14.28000   Max.   :5.8800   Max.   :12.50000   Max.   :4.76000  \n",
       "      data              num415            num85           technology     \n",
       " Min.   : 0.00000   Min.   :0.00000   Min.   : 0.0000   Min.   :0.00000  \n",
       " 1st Qu.: 0.00000   1st Qu.:0.00000   1st Qu.: 0.0000   1st Qu.:0.00000  \n",
       " Median : 0.00000   Median :0.00000   Median : 0.0000   Median :0.00000  \n",
       " Mean   : 0.09723   Mean   :0.04784   Mean   : 0.1054   Mean   :0.09748  \n",
       " 3rd Qu.: 0.00000   3rd Qu.:0.00000   3rd Qu.: 0.0000   3rd Qu.:0.00000  \n",
       " Max.   :18.18000   Max.   :4.76000   Max.   :20.0000   Max.   :7.69000  \n",
       "    num1999          parts              pm               direct       \n",
       " Min.   :0.000   Min.   :0.0000   Min.   : 0.00000   Min.   :0.00000  \n",
       " 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.: 0.00000   1st Qu.:0.00000  \n",
       " Median :0.000   Median :0.0000   Median : 0.00000   Median :0.00000  \n",
       " Mean   :0.137   Mean   :0.0132   Mean   : 0.07863   Mean   :0.06483  \n",
       " 3rd Qu.:0.000   3rd Qu.:0.0000   3rd Qu.: 0.00000   3rd Qu.:0.00000  \n",
       " Max.   :6.890   Max.   :8.3300   Max.   :11.11000   Max.   :4.76000  \n",
       "       cs             meeting           original         project       \n",
       " Min.   :0.00000   Min.   : 0.0000   Min.   :0.0000   Min.   : 0.0000  \n",
       " 1st Qu.:0.00000   1st Qu.: 0.0000   1st Qu.:0.0000   1st Qu.: 0.0000  \n",
       " Median :0.00000   Median : 0.0000   Median :0.0000   Median : 0.0000  \n",
       " Mean   :0.04367   Mean   : 0.1323   Mean   :0.0461   Mean   : 0.0792  \n",
       " 3rd Qu.:0.00000   3rd Qu.: 0.0000   3rd Qu.:0.0000   3rd Qu.: 0.0000  \n",
       " Max.   :7.14000   Max.   :14.2800   Max.   :3.5700   Max.   :20.0000  \n",
       "       re               edu              table            conference      \n",
       " Min.   : 0.0000   Min.   : 0.0000   Min.   :0.000000   Min.   : 0.00000  \n",
       " 1st Qu.: 0.0000   1st Qu.: 0.0000   1st Qu.:0.000000   1st Qu.: 0.00000  \n",
       " Median : 0.0000   Median : 0.0000   Median :0.000000   Median : 0.00000  \n",
       " Mean   : 0.3012   Mean   : 0.1798   Mean   :0.005444   Mean   : 0.03187  \n",
       " 3rd Qu.: 0.1100   3rd Qu.: 0.0000   3rd Qu.:0.000000   3rd Qu.: 0.00000  \n",
       " Max.   :21.4200   Max.   :22.0500   Max.   :2.170000   Max.   :10.00000  \n",
       " charSemicolon     charRoundbracket charSquarebracket charExclamation  \n",
       " Min.   :0.00000   Min.   :0.000    Min.   :0.00000   Min.   : 0.0000  \n",
       " 1st Qu.:0.00000   1st Qu.:0.000    1st Qu.:0.00000   1st Qu.: 0.0000  \n",
       " Median :0.00000   Median :0.065    Median :0.00000   Median : 0.0000  \n",
       " Mean   :0.03857   Mean   :0.139    Mean   :0.01698   Mean   : 0.2691  \n",
       " 3rd Qu.:0.00000   3rd Qu.:0.188    3rd Qu.:0.00000   3rd Qu.: 0.3150  \n",
       " Max.   :4.38500   Max.   :9.752    Max.   :4.08100   Max.   :32.4780  \n",
       "   charDollar         charHash          capitalAve        capitalLong     \n",
       " Min.   :0.00000   Min.   : 0.00000   Min.   :   1.000   Min.   :   1.00  \n",
       " 1st Qu.:0.00000   1st Qu.: 0.00000   1st Qu.:   1.588   1st Qu.:   6.00  \n",
       " Median :0.00000   Median : 0.00000   Median :   2.276   Median :  15.00  \n",
       " Mean   :0.07581   Mean   : 0.04424   Mean   :   5.191   Mean   :  52.17  \n",
       " 3rd Qu.:0.05200   3rd Qu.: 0.00000   3rd Qu.:   3.706   3rd Qu.:  43.00  \n",
       " Max.   :6.00300   Max.   :19.82900   Max.   :1102.500   Max.   :9989.00  \n",
       "  capitalTotal          type     \n",
       " Min.   :    1.0   nonspam:2788  \n",
       " 1st Qu.:   35.0   spam   :1813  \n",
       " Median :   95.0                 \n",
       " Mean   :  283.3                 \n",
       " 3rd Qu.:  266.0                 \n",
       " Max.   :15841.0                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "nonspam    spam \n",
       "   2788    1813 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\ngZBgAiHBBP8LIDSquGv7WNcAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(spam$type)\n",
    "plot(spam$type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram (or other relevant) plots of the variables with respect to the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'ggplot2'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:kernlab':\n",
      "\n",
      "    alpha\n",
      "\n",
      "\n",
      "Warning message:\n",
      "\"package 'GGally' was built under R version 4.0.4\"\n",
      "Registered S3 method overwritten by 'GGally':\n",
      "  method from   \n",
      "  +.gg   ggplot2\n",
      "\n",
      "`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n",
      "\n",
      "`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n",
      "\n",
      "`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n",
      "\n",
      "`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAnFBMVEUAAAAAv8QaGhozMzNN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\ngAASBAUQQIKgAAJIEBRA/w8xj3MNNASfrAAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(\"ggplot2\"); library(\"GGally\")\n",
    "ggpairs(spam[,c(21,23,7,57,58)],aes(colour = type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification trees \n",
    "\n",
    "### Model structure\n",
    "\n",
    "A classification tree is a model employing a tree-based structure in order to perform classification of the target variable.\n",
    "The tree structure contains internal nodes, and terminal nodes (often called leaves).\n",
    "\n",
    "Each internal node is associated to a feature in the input vector $\\mathbf{x}$, and performs a partition of the input space according to the value of the associated feature.\n",
    "Each terminal node is associated to a specific output label $C_i$.\n",
    "Internal nodes can be concatenated with each other in order to specify additional partitions of the input space.\n",
    "\n",
    "By combining these definitions, we can notice that the terminal nodes partitions the input space into mutually exclusive regions (in a divide-and-conquer fashion). \n",
    "\n",
    "The classification operation is performed by traversing the tree from the root node until one of the terminal nodes is reached.\n",
    "At each intermediate internal node, a decision is made according to the value associated to the corresponding feature (for example, for the i-th feature $x_i \\geq 0$).\n",
    "According to the outcome of this decision, a different path in the tree is taken.\n",
    "The tree traversal is stopped once a terminal node is reached, and the output of the classifier correspond to the label associated to the terminal node (e.g. $C_0$).\n",
    "\n",
    "Hence, by simply looking at the tree structure, we can easily understand the sequence of operations (i.e. splits) that yielded to the classification decision, improving interpretability of the model.\n",
    "\n",
    "### Learning procedure\n",
    "\n",
    "As an analogy to their biological equivalent, the learning procedure for a decision tree has two steps known as *tree growing* and *tree pruning*.\n",
    "\n",
    "* During *tree growing*, an iterative, exhaustive search is performed to find the successive splits, selecting the one that minimizes a certain cost function.\n",
    "  \n",
    "  Here you can find two examples of commonly used cost functions, with $p_i$ representing the probability of selecting a sample of class $i$ in the input space subset defined by the considered split:\n",
    "\n",
    "    * The **Gini Impurity** (used in CART) : how often a randomly chosen element would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. It is minimum (zero) for class-homogeneous nodes.\n",
    "    \n",
    "    \\begin{equation}\n",
    "         Gini = \\sum_{i=1}^C p_i * (1-p_i) = 1-\\sum_{i=1}^C p_i^2\\\\\n",
    "    \\end{equation}\n",
    "\n",
    "    * The **information gain** (IG, used in ID3,C4.5) : based on the concept of entropy and information content from information theory. Basically the IG is the entropy of the parent node minus the weighted sum of the children node entropy.\n",
    "\n",
    "    \\begin{equation}\n",
    "         IG = -\\sum_{i=1}^C p_i * log_2 (p_i) - \\sum_{a} p(a) \\sum_{i=1}^C Pr(i|a) * log_2 (Pr(i|a))  \\\\\n",
    "    \\end{equation}\n",
    "   \n",
    "\n",
    "* During *tree pruning*, some branches of the decision tree are removed, based on a complexity based measure of the tree performance, in order to avoid overfitting.\n",
    "\n",
    "\n",
    "An animated version of decision trees can be found here : http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Classification with decision trees\n",
    "\n",
    "On the given dataset, our classification task will be to discriminated between spam and non-spam samples ($k=2$ classes - binary classificiation task).\n",
    "Among the difference performance metric presented, we will be using the misclassification rate. \n",
    "\n",
    "In order to study the generalization capabilities of our learning model, we will be dividing our dataset in two part :\n",
    "\n",
    "* *training set* : Sample of data that we will be using to perform the training of the model, presenting to the model the $(\\mathbf{x}_i,y_i)$ pairs.\n",
    "* *test set* : Sample of data, unseen by the model, used to assess the predictions made by the training model.\n",
    "\n",
    "In order to obtain a more statistically sound estimation of the performance of the model, this procedure is often repeated several times, with different training-test splits (k-fold cross-validation).\n",
    "\n",
    "For this exercise, we will ask you to:\n",
    "\n",
    "* First, obtain the performance of a DT on the spam dataset using a 50%/50% training-test partition. \n",
    "* Check the importance of each feature using the obtained DT using *rpart* or *tree*.\n",
    "* Then plot the obtained tree using packages *rpart.plot*.\n",
    "* Finally, implement a 10-fold cross-validation for assessing your DT.\n",
    "\n",
    "**N.B.** The selected classifier actually outputs an estimation of the conditional probabilities $p(y_i=C_0|\\mathbf{x})$ and $p(y_i=C_1|\\mathbf{x_i})$, $C_0 and C_1$ being respectively \"nonspam\" and \"spam\". In order to transform the conditional probability into the corresponding class, we will define a threshold $t=0.5$. If $p(y_i=C_1|\\mathbf{x_i}) > t$ than the sample $\\mathbf{x_i}$ will be affected to the class \"spam\", otherwise it will be affected to the \"non spam\" category.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(rpart)\n",
    "\n",
    "spam_idx <- sample(1:nrow(spam))\n",
    "half_split <- floor(nrow(spam)/2)\n",
    "target_variable <- ncol(spam)\n",
    "\n",
    "train_data <- spam[spam_idx[1:half_split],]\n",
    "test_data <- spam[spam_idx[(half_split+1):nrow(spam)],]\n",
    "\n",
    "model <- rpart(type ~ ., method=\"class\",data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification tree:\n",
      "rpart(formula = type ~ ., data = train_data, method = \"class\")\n",
      "\n",
      "Variables actually used in tree construction:\n",
      "[1] capitalAve      charDollar      charExclamation free           \n",
      "[5] hp              remove         \n",
      "\n",
      "Root node error: 903/2300 = 0.39261\n",
      "\n",
      "n= 2300 \n",
      "\n",
      "        CP nsplit rel error  xerror     xstd\n",
      "1 0.450720      0   1.00000 1.00000 0.025935\n",
      "2 0.083056      1   0.54928 0.55814 0.021969\n",
      "3 0.050941      3   0.38317 0.39424 0.019210\n",
      "4 0.035437      4   0.33223 0.34884 0.018259\n",
      "5 0.024363      5   0.29679 0.33887 0.018037\n",
      "6 0.021041      6   0.27243 0.32337 0.017682\n",
      "7 0.014396      7   0.25138 0.29125 0.016901\n",
      "8 0.010000      8   0.23699 0.26910 0.016326\n",
      "Call:\n",
      "rpart(formula = type ~ ., data = train_data, method = \"class\")\n",
      "  n= 2300 \n",
      "\n",
      "          CP nsplit rel error    xerror       xstd\n",
      "1 0.45071982      0 1.0000000 1.0000000 0.02593525\n",
      "2 0.08305648      1 0.5492802 0.5581395 0.02196933\n",
      "3 0.05094131      3 0.3831672 0.3942414 0.01920974\n",
      "4 0.03543743      4 0.3322259 0.3488372 0.01825929\n",
      "5 0.02436323      5 0.2967885 0.3388704 0.01803730\n",
      "6 0.02104097      6 0.2724252 0.3233666 0.01768160\n",
      "7 0.01439646      7 0.2513843 0.2912514 0.01690136\n",
      "8 0.01000000      8 0.2369878 0.2691030 0.01632558\n",
      "\n",
      "Variable importance\n",
      "charExclamation      charDollar     capitalLong            free            your \n",
      "             21              15              11               9               9 \n",
      "            you          remove          num000    capitalTotal           money \n",
      "              8               7               3               3               3 \n",
      "           make              hp      capitalAve             hpl          telnet \n",
      "              2               2               2               1               1 \n",
      "           labs          num650 \n",
      "              1               1 \n",
      "\n",
      "Node number 1: 2300 observations,    complexity param=0.4507198\n",
      "  predicted class=nonspam  expected loss=0.3926087  P(node) =1\n",
      "    class counts:  1397   903\n",
      "   probabilities: 0.607 0.393 \n",
      "  left son=2 (1159 obs) right son=3 (1141 obs)\n",
      "  Primary splits:\n",
      "      charExclamation < 0.0045 to the left,  improve=369.7542, (0 missing)\n",
      "      charDollar      < 0.0555 to the left,  improve=344.2635, (0 missing)\n",
      "      remove          < 0.01   to the left,  improve=310.6648, (0 missing)\n",
      "      free            < 0.085  to the left,  improve=286.5758, (0 missing)\n",
      "      your            < 0.375  to the left,  improve=276.6745, (0 missing)\n",
      "  Surrogate splits:\n",
      "      your        < 0.255  to the left,  agree=0.723, adj=0.442, (0 split)\n",
      "      capitalLong < 15.5   to the left,  agree=0.694, adj=0.384, (0 split)\n",
      "      you         < 0.775  to the left,  agree=0.686, adj=0.366, (0 split)\n",
      "      charDollar  < 0.0335 to the left,  agree=0.683, adj=0.361, (0 split)\n",
      "      free        < 0.015  to the left,  agree=0.677, adj=0.350, (0 split)\n",
      "\n",
      "Node number 2: 1159 observations,    complexity param=0.03543743\n",
      "  predicted class=nonspam  expected loss=0.1113028  P(node) =0.503913\n",
      "    class counts:  1030   129\n",
      "   probabilities: 0.889 0.111 \n",
      "  left son=4 (1105 obs) right son=5 (54 obs)\n",
      "  Primary splits:\n",
      "      remove     < 0.035  to the left,  improve=53.15177, (0 missing)\n",
      "      charDollar < 0.149  to the left,  improve=49.47062, (0 missing)\n",
      "      money      < 0.01   to the left,  improve=43.56226, (0 missing)\n",
      "      free       < 0.28   to the left,  improve=34.37477, (0 missing)\n",
      "      your       < 2.28   to the left,  improve=20.60383, (0 missing)\n",
      "  Surrogate splits:\n",
      "      num3d    < 13.115 to the left,  agree=0.955, adj=0.037, (0 split)\n",
      "      num000   < 1.125  to the left,  agree=0.954, adj=0.019, (0 split)\n",
      "      charHash < 1.0465 to the left,  agree=0.954, adj=0.019, (0 split)\n",
      "\n",
      "Node number 3: 1141 observations,    complexity param=0.08305648\n",
      "  predicted class=spam     expected loss=0.3216477  P(node) =0.496087\n",
      "    class counts:   367   774\n",
      "   probabilities: 0.322 0.678 \n",
      "  left son=6 (586 obs) right son=7 (555 obs)\n",
      "  Primary splits:\n",
      "      charDollar  < 0.0085 to the left,  improve=115.88550, (0 missing)\n",
      "      capitalAve  < 2.0745 to the left,  improve=109.63640, (0 missing)\n",
      "      hp          < 0.12   to the right, improve=106.75660, (0 missing)\n",
      "      capitalLong < 17.5   to the left,  improve= 96.29872, (0 missing)\n",
      "      remove      < 0.01   to the left,  improve= 88.67572, (0 missing)\n",
      "  Surrogate splits:\n",
      "      num000       < 0.015  to the left,  agree=0.748, adj=0.481, (0 split)\n",
      "      capitalTotal < 179.5  to the left,  agree=0.739, adj=0.463, (0 split)\n",
      "      money        < 0.045  to the left,  agree=0.713, adj=0.411, (0 split)\n",
      "      capitalLong  < 18.5   to the left,  agree=0.707, adj=0.398, (0 split)\n",
      "      make         < 0.045  to the left,  agree=0.695, adj=0.373, (0 split)\n",
      "\n",
      "Node number 4: 1105 observations,    complexity param=0.01439646\n",
      "  predicted class=nonspam  expected loss=0.07782805  P(node) =0.4804348\n",
      "    class counts:  1019    86\n",
      "   probabilities: 0.922 0.078 \n",
      "  left son=8 (1070 obs) right son=9 (35 obs)\n",
      "  Primary splits:\n",
      "      charDollar < 0.1725 to the left,  improve=26.712910, (0 missing)\n",
      "      money      < 0.01   to the left,  improve=19.031870, (0 missing)\n",
      "      capitalAve < 3.7405 to the left,  improve=11.875930, (0 missing)\n",
      "      free       < 0.28   to the left,  improve=10.150240, (0 missing)\n",
      "      num000     < 0.215  to the left,  improve= 9.721472, (0 missing)\n",
      "  Surrogate splits:\n",
      "      num000 < 0.68   to the left,  agree=0.971, adj=0.086, (0 split)\n",
      "      money  < 0.235  to the left,  agree=0.970, adj=0.057, (0 split)\n",
      "\n",
      "Node number 5: 54 observations\n",
      "  predicted class=spam     expected loss=0.2037037  P(node) =0.02347826\n",
      "    class counts:    11    43\n",
      "   probabilities: 0.204 0.796 \n",
      "\n",
      "Node number 6: 586 observations,    complexity param=0.08305648\n",
      "  predicted class=nonspam  expected loss=0.4590444  P(node) =0.2547826\n",
      "    class counts:   317   269\n",
      "   probabilities: 0.541 0.459 \n",
      "  left son=12 (482 obs) right son=13 (104 obs)\n",
      "  Primary splits:\n",
      "      remove     < 0.08   to the left,  improve=71.39361, (0 missing)\n",
      "      free       < 0.185  to the left,  improve=53.35643, (0 missing)\n",
      "      capitalAve < 2.6695 to the left,  improve=48.00557, (0 missing)\n",
      "      hp         < 0.11   to the right, improve=45.33473, (0 missing)\n",
      "      our        < 0.255  to the left,  improve=35.96671, (0 missing)\n",
      "  Surrogate splits:\n",
      "      capitalLong < 133    to the left,  agree=0.840, adj=0.096, (0 split)\n",
      "      charHash    < 0.889  to the left,  agree=0.833, adj=0.058, (0 split)\n",
      "      capitalAve  < 13.66  to the left,  agree=0.833, adj=0.058, (0 split)\n",
      "      receive     < 0.115  to the left,  agree=0.831, adj=0.048, (0 split)\n",
      "      num3d       < 4.265  to the left,  agree=0.826, adj=0.019, (0 split)\n",
      "\n",
      "Node number 7: 555 observations,    complexity param=0.02104097\n",
      "  predicted class=spam     expected loss=0.09009009  P(node) =0.2413043\n",
      "    class counts:    50   505\n",
      "   probabilities: 0.090 0.910 \n",
      "  left son=14 (25 obs) right son=15 (530 obs)\n",
      "  Primary splits:\n",
      "      hp     < 0.37   to the right, improve=32.66948, (0 missing)\n",
      "      hpl    < 0.165  to the right, improve=27.41518, (0 missing)\n",
      "      telnet < 0.05   to the right, improve=20.30959, (0 missing)\n",
      "      num650 < 0.025  to the right, improve=15.53914, (0 missing)\n",
      "      labs   < 0.04   to the right, improve=15.53914, (0 missing)\n",
      "  Surrogate splits:\n",
      "      hpl    < 0.35   to the right, agree=0.984, adj=0.64, (0 split)\n",
      "      telnet < 0.05   to the right, agree=0.977, adj=0.48, (0 split)\n",
      "      num650 < 0.025  to the right, agree=0.971, adj=0.36, (0 split)\n",
      "      labs   < 0.04   to the right, agree=0.971, adj=0.36, (0 split)\n",
      "      george < 0.505  to the right, agree=0.964, adj=0.20, (0 split)\n",
      "\n",
      "Node number 8: 1070 observations\n",
      "  predicted class=nonspam  expected loss=0.05794393  P(node) =0.4652174\n",
      "    class counts:  1008    62\n",
      "   probabilities: 0.942 0.058 \n",
      "\n",
      "Node number 9: 35 observations\n",
      "  predicted class=spam     expected loss=0.3142857  P(node) =0.01521739\n",
      "    class counts:    11    24\n",
      "   probabilities: 0.314 0.686 \n",
      "\n",
      "Node number 12: 482 observations,    complexity param=0.05094131\n",
      "  predicted class=nonspam  expected loss=0.3443983  P(node) =0.2095652\n",
      "    class counts:   316   166\n",
      "   probabilities: 0.656 0.344 \n",
      "  left son=24 (398 obs) right son=25 (84 obs)\n",
      "  Primary splits:\n",
      "      free            < 0.4    to the left,  improve=37.51630, (0 missing)\n",
      "      charExclamation < 0.5645 to the left,  improve=33.98041, (0 missing)\n",
      "      capitalAve      < 2.6605 to the left,  improve=33.68480, (0 missing)\n",
      "      hp              < 0.015  to the right, improve=25.55738, (0 missing)\n",
      "      capitalLong     < 17.5   to the left,  improve=20.75853, (0 missing)\n",
      "  Surrogate splits:\n",
      "      num3d   < 0.935  to the left,  agree=0.830, adj=0.024, (0 split)\n",
      "      receive < 0.66   to the left,  agree=0.830, adj=0.024, (0 split)\n",
      "      num000  < 0.3    to the left,  agree=0.830, adj=0.024, (0 split)\n",
      "      our     < 2.04   to the left,  agree=0.828, adj=0.012, (0 split)\n",
      "      email   < 4.22   to the left,  agree=0.828, adj=0.012, (0 split)\n",
      "\n",
      "Node number 13: 104 observations\n",
      "  predicted class=spam     expected loss=0.009615385  P(node) =0.04521739\n",
      "    class counts:     1   103\n",
      "   probabilities: 0.010 0.990 \n",
      "\n",
      "Node number 14: 25 observations\n",
      "  predicted class=nonspam  expected loss=0.12  P(node) =0.01086957\n",
      "    class counts:    22     3\n",
      "   probabilities: 0.880 0.120 \n",
      "\n",
      "Node number 15: 530 observations\n",
      "  predicted class=spam     expected loss=0.05283019  P(node) =0.2304348\n",
      "    class counts:    28   502\n",
      "   probabilities: 0.053 0.947 \n",
      "\n",
      "Node number 24: 398 observations,    complexity param=0.02436323\n",
      "  predicted class=nonspam  expected loss=0.2537688  P(node) =0.1730435\n",
      "    class counts:   297   101\n",
      "   probabilities: 0.746 0.254 \n",
      "  left son=48 (330 obs) right son=49 (68 obs)\n",
      "  Primary splits:\n",
      "      capitalAve      < 3.6865 to the left,  improve=27.30358, (0 missing)\n",
      "      charExclamation < 0.5085 to the left,  improve=17.53048, (0 missing)\n",
      "      hp              < 0.015  to the right, improve=16.32587, (0 missing)\n",
      "      capitalLong     < 17.5   to the left,  improve=14.89854, (0 missing)\n",
      "      re              < 0.025  to the right, improve=12.68116, (0 missing)\n",
      "  Surrogate splits:\n",
      "      capitalLong   < 51.5   to the left,  agree=0.874, adj=0.265, (0 split)\n",
      "      font          < 0.335  to the left,  agree=0.862, adj=0.191, (0 split)\n",
      "      charHash      < 0.395  to the left,  agree=0.849, adj=0.118, (0 split)\n",
      "      charSemicolon < 0.302  to the left,  agree=0.842, adj=0.074, (0 split)\n",
      "      order         < 1.095  to the left,  agree=0.837, adj=0.044, (0 split)\n",
      "\n",
      "Node number 25: 84 observations\n",
      "  predicted class=spam     expected loss=0.2261905  P(node) =0.03652174\n",
      "    class counts:    19    65\n",
      "   probabilities: 0.226 0.774 \n",
      "\n",
      "Node number 48: 330 observations\n",
      "  predicted class=nonspam  expected loss=0.169697  P(node) =0.1434783\n",
      "    class counts:   274    56\n",
      "   probabilities: 0.830 0.170 \n",
      "\n",
      "Node number 49: 68 observations\n",
      "  predicted class=spam     expected loss=0.3382353  P(node) =0.02956522\n",
      "    class counts:    23    45\n",
      "   probabilities: 0.338 0.662 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAf6klEQVR4nO3di5qaOgBF4SCIjnJ5/7ct4A2rIpCdkMT1f6c9dlpDBl0jIKpp\nAVgzW08ASAEhAQKEBAgQEiBASIAAIQEChBQiM/tmafbGlNfLJ0ezwQyEFKL5IRXGmMPl4o7b\nckOs/LgZUz8ubjmRX8fKj9uoHkLaEis/IKfcmLzf0+mbMFfdH487kx3H/7D7wu7Y3v7N5WvX\ni8Y0O1M8X+nl+pAjpHAcL10c/w+pGP6fP/5hfvvC25CK4fDD6Eov14ceIYUjM1Xb/pndaCtt\n393/TyZv2iY394Nyfyar2iozf+2bTTvT/+t2fKWX68MBQgqHud/Xb3nk/eNIYfoymmF7bVAM\n/+40PMa8Cel8+Tf3K71cHw4QUjjKbrOsqvpL1zzyaytmvAl3/9vhf29Cuv7hfqWX68MB1m5A\nDll3d8/qWw252ff/I6QYsHaDcip3t32kOruesvBSwLyQ/v/ncIqVHJrLg8ijo+su0chtH6lo\nP4c0utLL9eEAIYVj1x+Hux21u3d0OUjXHh8HCz4etasfXxhd6eX6cICQwvF32ZU5DzWM9mwu\nTxtl93OBHs8jPYW06/7N+IDf/Uov14ceIQVkOLOhP3r9HFJ/ZoLZjzs4ZpczG55COu/GIY2v\n9HJ9yBESIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKE\nBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQI\nEBIgsGFIR2fLPu5MVjauRj87mvfjE/r0qr2rz+wbf7KgWlNmzm7Ibuhc+SHV24VUOfvY+vLy\nmamOSmoyR/d1hyGd3K2RW0eZfug6c/fht5fP1T3oBtwspCpzFVJl9k3/eLd3M3zh6kHD4ceO\nZ1nVNsX9c9L1Tv1HSKvthwmXLm7Io8mbttmbSjbiViF134mrkIrLuI6G/3O19XVU/nx89jfc\nIxsXjxoXTebih4Bxd0PmQ/i18EfLViF134KzTbvrEpwMXzv7AXA0Ryfjtv2Pdt1P3rcK42Kr\n8boN7aL/W6O5bkTZSMtUzh4yrhrhShrJTe1o3oU57btdaxdD70x7yIYNXjcqNxuNh+umnYNH\nav2D3YZH7dyGdDTKYzI3B/Pnat7FZafdRf7GFI6OB1y4eUDqbsP+aEPm4oF6Z/ojGGdC+qp2\nstU+HBBwNG/TNdo2pYsNvO7eWPW71o52wipXx3UO6kNro5GLpq2UW+mJhtRkTjbsdv0RZKc/\nABqz0w9qhn2k2sXQvdLJg3+/UdFt2nX9u3hIGo6sKw/AJhpS7uQ+sx/uMW43SV2M7vDwV8/R\nE2vd9le/xejkR0vfZ3ZgH+mLepe7fRbf4VpzMbbbJwScPQHmuP9+5rpGUwzp5OaAneOQsuHn\nb+3iXnkYHklrR6vF2XH7yyOdk6e/Liv7KFzZCYbk6g5z42je5bBH4GR/o9s7Gp7H/9MP3faP\nd46eperWSHNdL/qh92173gnXSIIh7R1vfzkauLmcWeboGRlXR9bb266MC7mzaV9XtvDRP8GQ\nXO/IuBq4P9d552gr6ZQ7eq63dXrwZTj728nIdffjtkjj7G8gIYQECBASIEBIgAAhAQKEBAgQ\nEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgMCWIblctsu3J3E3NNOOdmhCCmloph3t\n0IQU0tBMO9qhCSmkoZl2tEMTUkhDM+1ohyakkIZm2tEOTUghDc20ox2akEIammlHOzQhhTQ0\n0452aEIKaWimHe3QhBTS0Ew72qEJKaShmXa0Q3sIyQCRWXEv14ezwSIAJUICBAgJECAkQICQ\nAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAAB\nQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUIC\nBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQI\nCRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQ\nICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkACByEKiMISJkAABryGdD4XpFeV55SII\nCWHyGFKzMw/5ukUQEsLkMaTSZH/VcKk+ZaZctQhCQpg8hpSZ6n65MtmqRRASwuQxJGM+/WH+\nIggJYeIRCRDwu490qodL7CMhNT4Pf+ejo3a7ZtUiCAlh8vs8Ujk8j5QVB55HQlriOrPBTB+j\nALYSU0hdRebL4T5gG1GFdPsPCM1WIa14HsmMfgFhCSckM/ZpuYSEMEW0aUdICFdEIQ07SIaO\nEKKoQjLtp80+YFsxhcTzSAhWXCG1bNkhTIQECHh9PdLXI9xfF0FICJPHkI6SkCgJIfK5aVdl\n0295MmMRhIQwed1HqqZfzjdjETwhizD5PdhwHL3afNUiCAlhiu6oHSEhRBGGREkIDyEBAoQE\nCEQWkrcZAIsQEiBASIBAlCFREkJDSIAAIQECcYZESQgMIQEChAQIRBoSJSEshAQIxBoSJSEo\nhAQIEBIgEG1IlISQEBIgQEiAQLwhURICQkiAACEBAhGHREkIByEBAoQECMQcEiUhGIQECEQd\nEiUhFIQECBASIBB3SJSEQBASIEBIgEDkIVESwkBIgAAhAQKxh0RJCAIhAQKEBAhEHxIlIQSE\nBAgQEiAQf0iUhAAQEiCQQEiUhO0REiBASIBACiFREjZHSIAAIQECSYRESdgaIQEChAQIpBES\nJWFjhAQIEBIgkEhIlIRtERIgQEiAQCohURI2FeZdNsxZAR+FeZddswhKwoYICRAgJEAgnZAo\nCRsiJECAkACBhEKiJGyHkAABQgIEUgqJkrAZQgIECAkQSCokSsJWCAkQICRAIK2QKAkbISRA\nILGQKAnbICRAgJAAgdRCoiRsgpAAAUICBJILiZKwBUICBAgJEEgvJErCBggJECAkQCDBkCgJ\n/vkMqd6b7NC2x53JSkeLEFwbWMFjSE1mOsdD/7vJnSxCc3VgMY8hlaZ7HCozs2/aZrisX4Tm\n6sBiHkPKhisa0wz/y1wsQnN1YDGPIRnz+P32P/EiVNcHFtrgEan/vXH6iERI8G2DfaSyuV7W\nL0J1fWChJI/aURJ8S/F5JMUAwCIpntmgGABYJNGQKAl+ERIgsFVIbp9HIiR4Fk5IZkywAPsh\ngNlS3bQjJHhFSIBAsiFREnzyGtL5UAx7QEV5drUI7RjATD5PEdqNjiY4PkVINggwi9eTVrO/\narhUnzK3J63qBgFm8foyiup+uXL7MgrdIMAsliEVX84+fbqe+fSHyUWsR0nwxjKkJU+d+n5E\nIiT4YxnS7vIODLN0+0inerjkZx+JkOCPZUhNkX85kj2Sj47a7SYDFCVASfDFetNuyelx53J4\nHikrDh6eR9INA3zlNaRVi9h+GOCrdE8RUo4DfEFIgIB1SH/9IYTiTzSdt4vYfhzgC9uQbkfi\nps+ds1pEEAMBkyxDOprs1P3vlJmjakb/LyKMgYBJ1k/IXs5WqMxOM5/XRYQxEDBJdYpQkIe/\npSMBE2SPSJPnztksIpSRgAmJ7yNREvxI/KgdIcEP++eRioCfR9IOBXyU9pkN6rGADzy+Qnbl\nIkIaC/jA4ytkVy4ipLGADzy+QnblIsIaDHjL5ytk1y0irMGAt5J+YZ+DwYC3fiAkSoJ76R/+\nJiR4kP7hb0KCB+kf/tYPB7z4gcPfhAT3fuDwNyHBvV84akdJcI6QAIFfOPztYkDgCSEBAhYh\nLfjgsLWLCHdA4Il1SNeCAg+JkuAWIQEChAQI/EpIlASnCAkQICRA4GdCoiS4ZBXSk41ntcmY\nwBUhAQI/coqQs0GBASEBAj8UEiXBHUICBAgJEPilkCgJzhASIEBIgMBPhURJcMU6pFPRn9RQ\n1KL5vFtE+OPi59mGlF/ODjKZtCRCQmQsQzqavOlDOpq9bEqtwzs8JcENy5Ay01xeQRH+Satu\nB8aPswxp2KwjJPw8y5B210ekyuxkU2pd3t8pCU5o9pFOmTnKptQSEqJje9SuuL6sL1dN6HUR\nkYyMnyZ5HskUf6LpvF1ELEPjh/3WmQ1uh8YPswzJyQdfur23UxIcsD38nZ9kU/mwiJjGxs+y\nPvxtTKn/FFlCQmRs95HqQ9fS7iDexHN6Z6ck6AkONtRlZsSbeISEyGiO2h0jeINIP4PjRyke\nkYatO+kzSW7v65QEOck+UlZqX9dHSIiN4KjdPq6jdoQEB6yfRxKfHPS6iPiGxw/6vTMb3A+P\nH2QR0uVFfdF8rIu/4fGDfjIkSoLaz5397WV8/BxCAgQEb34yyDLFbN4twglKgpYopDqufSRC\ngphFSKenz2KO5F2E/C0BP8XmEWk37kh6egMhITKqfSQtQkJkfvOonZ9F4IeoQjoXtjP5uggt\nQoKSbUhllGc2+FkEfohlSI+O4nmpucdl4GdYhpSZvzY3dZ2byI7aERKkBEftDt2jUaV9829C\nQmQEIZ36T6KIbh+JkqBkGVLRbdrVZteeCQk/zTKkUx/Q8IHMcXyGrPeF4EfYHv4+9H/aG1OK\n5vNmEc5QEmR+9swGb0vBTyAkQMDqPRuebDyrgBeDH0BIgMAvb9pREmR8htSU/Rs79G8W/u39\nWQkJkbEOqf9U87YtZryLfp11/7LJLhuC02cUERIiYxtSftk9Mtn3kvamaLrf9t2/rPfTTzx5\nu4NTEjQsQzqavOlDOs44s8GY5vpbt5VnJt++i5AQGeuXUTSX81VnHLUb/klmRn9QzmodQoKG\n4OzvuSHtTdWfUlT1l5vpnSR/929KgoRlSLvrI1I1433tKpOVVVtkXUmn3fQragkJkdHsI52y\n/jVJ35yyx9O3B/WsViIkSNgetSvMnMPZN3/74U0li8OXY3we796UBAXJ80imEH8AJiEhMr99\nipDfRSFhPx8SJUFBFVK18J1WA3keyfOykCybkM65MfnwtFBVLH0Zxeu/d/aajK9T8bkwJMoi\npPPlPl+1dX+8QfqmDYSEyFiElPfxlCbvP3CsaLaeVSQLQ6KsXiF7+T0zRSWc0XgRflASrAlC\nmv9hfefD5fnbovxyFUJCZAQhzb1eM/6ozDBe2LfF0pAkjyGVJvu7bAPWpyyQF/ZtsjgkyGNI\nmXnsSlWBvLBvk8UhQR7fjuvpn4TzhKz/xSFBHkMK9xGJkmDL47l23T7S6fLyidD2kQgJtnye\ntJqPHr92k8/gbhISNWE9r2d/n8vheaSsOAT1PNJ1gYSE9XgZxX2BhIT1COm+QELCeoR0XyIh\nYT1Cui+RkLAeId0XSUhYj5DuiyQkrOfxzAa3s7JfJCFhPUK6L9PvO0UgLWzaXZfYd0RKWIuQ\nbks0HLjDeqqQzgvf127FIlwyo1/AcrYhlWnsIxES7FiG9Oho8vOObBbhxRAR23ZYzTKkzPy1\nuanr3Mx+K6Gli/DD3HaTgDUsQ+rveofu0aia+QFJKxbhR3/MjpCwmiCkU/9pfZHvI7XX55Eo\nCetYhlR0m3a12fXvAy6bUrvR/ZmXyWI9y5BOfUDDS8j3sim1W4ZESVjF9vD3of/TXvxhFFuG\nRElYgzMbXhZKSVjOMiTtp7m8XYQ3hIT1bI/a5dInYt8twhvzcgGYyzKk/gMmvn1GywrbhkRJ\nWMx2H6k+dC3tDuJNvI1DoiQsJTjYUJeZEW/ibR0SJWEhzVG7Y+xnfwc4A0RF8Yg0bN39Sabz\nYREbCGAKiIhkHykra9V83ixiIyHMAdEQHLXbJ3LU7n9BTAKRsH4eSbpJ924R2wljFogCZzZM\nCGQaiIDiqJ3+9XCh3INDmQeCR0iTgpkIAkdI08KZCYJGSF8ENBUEjJC+CWkuCBYhfRPSXBAs\n2Stk43/Phk+CmgwCZRHSOJ1zlthJq2NhzQZBsgjJZPdzg/Ym9jeInBTYdBAgi5Dy23twdQ9H\n/XtECoV2zw1tPgiOzT7SMRselPqHowTP/n4S3IQQGKuDDU3RPSjJH47aEO+34c0IQbE8ancw\nxkg/Yux1EYEIcEoIiFVIdT48ImXyl1KEeK8NcU4IhtU+krntIxUpvIvQN0FOCoHgqN18Yc4K\nQeB5pAUCnRYCwJkNS4Q6L2yOc+0WCXZi2JgsJKlw76/hzgybIqSFAp4aNkRICwU8NWyIkJYK\neW7YDCEtFvTksBFCWi7s2WEThLRC4NPDBghpjdDnB+8IaZXgJwjPCGmd8GcIrwhppQimCI8I\naa0Y5ghvCGm1KCYJTwhpvThmCS8IyUIk04QHhGQjlnnCOUKyEs1E4Rgh2YlnpnCKkOzEM1M4\nRUiWIpoqHCIkWzHNFc4QkrWoJgtHCMleXLOFE4QkENl04QAhKcQ2X8gRkkR0E4YYIWnEN2NI\nEZJIhFOGECGpxDhnyBCSTJSThggh6cQ5a0gQklCk04YAISnFOm9YIySpaCcOS4QkFe3EYYmQ\ntOKdOawQkljEU4cFQlKLee5YjZDkop48ViIkvbhnj1UIyYHIp48VCMmF2OePxTYJyXwbIvo7\nYvTfABYiJDfi/w6wiMeQzDMXiwhIAt8CFvAY0jn7pZCS+B4wm89Nu6YweT2MkP6mXZvIN4GZ\n/O4j/Rnz1/5KSKPvIo3vBxM8H2yoc1M0vxLS49tI5PvBZ96P2h1MdvqVkO7fRyrfDz7yf/i7\n2n050mC/iGAQ0s/Y4nmk/c+EdPtGkvl+8AmnCLllRr8jYYTkmLn/hpRtFVL6T8jemOGcjq1n\nAcfCCWn2aQ+x6b4b8/2IP+LGpp1zQ0hpfUt4QUiumevDb0rfE14QkmtDRDwkpc5rSOdDMewB\nFeXZ1SLCc3k0Sm3HD//xGFKzGx1NyJ0sIkjm/h8tpctjSKXJ/qrhUn3KTOliEUEaH7WjpVR5\nDCkz1f1yZTIXiwjUUz5s5CXJ60vNP/1BtohQ/f/90FJyeETy4c33wwNTWvzuI52GV5r/2D5S\n+/H7oaV0+Dz8nY+O2u0aJ4sI1Ofvh5YS4fd5pHJ4HikrDj/0PFJveoeQmBLAmQ0+fP1+aCl2\nhOTDnO+HB6aoEVJIaClahBQYWooTIYWHjbwIEVKYaCkyhBSsmQ9MrKsgEFLQZrTEugoCIYXu\nFz4CJwGEFIOpllhXQSCkSHxsiXUVBEKKx/uNPNZVEAgpLq8tOVxX3AzzEVJ0nh+YXD7hxM0w\nHyFFydzfTMXl2yFzM8xHSLEy5vE+X64W4WjcFBFSxIaNvH5dOVpf3AzzEVLMTPv4EA83w2Mm\nQoqZGf96phoe8xBS1D7vIwm64gz0BQgparOP2pnFYfH5aIsQUuTWP9p86crtAcHkbmJCip1m\nb+ilq/EvF1K7iQkpdk4O16kPW7xZhJthN0NIsXN4R7/8cpNUajcxIcXO4bbX//tIyqRSu4kJ\nKXbOQpo6amefVGo3MSHFzuHJ38uOqy8cfN2cgkVI+GjZzbDwWarEbmNCwkfrt9u+JJXgk72E\nhI8UxxTeJuX4yd4tEBI+0h7vfiTl+sneLRASPnL1FNU4qFQQEj5yeXrQ7aVUjhbhHSHBv9E+\nUioxERL8+++oXQoPTYSELbyWE3lNhIRNvL+J462JkLCJiZs4ypoICZv4dhPHFhMhYRNzbuKY\nHpoICZuYfRMvr2mTew8hYRMrzix3M7QIISEWc2siJJ+LQJxmxERIPheBeH15aCIkn4tA5D7X\nREg+F4EUvKtpm0PmhITYPdW01avYCQlJuMW01avYCQnJuD40bfIqdkJCSu7vsOx/wT6uEuAi\nkKT7+6r4zomQkJTnfSR/PQUakuEXv1b9urw90cvXzYev65a74l6uD2eDRSBVnx+B7B+fPl+X\nkJCab/cei54ICb9j5r1nTU+EhN+xsI0lPRESfse6jbZZPRESfofV0YTJniZKIySkRnDveduT\nmfww0BULWX6VABcBfPXck7n99/afrhh99bxCWgQw16On2683/2bFsFaTCmURwDLjmN787YoB\n7eYTyCKAZXhEAhTYRwIEOGoHSPA8EqDAmQ2AACEBAoQECBASIEBIgFOEBAgQEiBASIAAIQEC\nhAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQECPkNq9sbkp+sgk6MQEiLj\nMaQmG949ubgMQkhIiceQSnPsajpm+TAIISElHkPKLless11NSEiMx5Bu7TR5TkhIjMeQdqa5\nXcoJCWnxGNLR7K+XapMTEpLi8/B3ea/n9OWD2AkJkfH6hGxV3C7Ve0JCSjizARAgJECAkACB\nrULiYAOSEk5IZkyxCMAfNu0AAUICBAgJEPAa0vlQXF6SVJ5dLQLYhM8X9u1GRxNyJ4sANuL1\nhX3ZXzVcqk+ZKV0sAtiI1xf2VffLlclcLALYyAYv7Hv9g2wRwEZ4RAIE/O4jnerhEvtISI3P\nw9/56Kjdrpn6l4SEyPh9HqkcnkfKigPPIyEtnNkACBASIEBIgAAhAQKEBAgQEiBASIAAIQEC\nhAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQE\nCBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIBBoSEBkVtzL9eEE\nsWyHYzNtn2PHMjQhhTQ00452aEIKaWimHe3QhBTS0Ew72qEJKaShmXa0QxNSSEMz7WiHJqSQ\nhmba0Q5NSCENzbSjHZqQQhqaaUc7NCGFNDTTjnZoQgppaKYd7dCEFNLQTDvaoTk1GxAgJECA\nkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIENgjp/7cob/bG\nlJZjlpnJyubpS0fz+Lv8pBl19IWqm/W+7i+te9/1qbE75+t4x93LN2Y19Gi8Vetl3tj9Tbqv\nhEPfb851b3M/a+h1K/smgJCKbrUc7IbMh3W7G3+pui3m8ndrFvAy6ugLp+Fi1gwLWhHS1Nid\nJruMV94Xoxl6NN6q9TJz7Gy4uKyk6TVyuzlvHWX6oVet7LsAQjKmthzxbLKqrTJzfnyp+9Nl\nMUeTN/1PyKU/H9+MOv5C1l1siv6RtDKFYMb/faG4zL4y+6b/FvaioUfjrVovM8cuL78tWi/T\na+R+c16cxje1aug1K/shiJBsRyxNv4nyN/rx2t1NrsPmw+qqV2w8vow6+sLfMF7T/2A8rnm0\nmxp7+P9l9sXtx7Bo6NF4q9bLzLEz0yyd9fQaedycgyZb1OjModes7IetQuoehgqTHW6P1XYj\nFsNj2vihobt/mKf1YnL7UUdfGP0kP5qjYMZPX6if7zfLbtvpoe/jrVovM8e+Xlq0+TU59OPm\nvP7Vos2vJUNHGFJ22URXhGRefpZU/39xxSJerjn6ws60h2zYFOhuldO+20fVjd0/WtRPP4AX\n3dunh76Pt2q9zBx7UC77ATM5dPV84y7cvFgw9NKVPVrIqmtZuYbUbaIfh/09+027t3eK6592\nw0+fszgkY4rbLm9x+VGwbPVP3rYH8/f0vRzNkoNr3+/sw3ir1svMsdth41R4b3/6i6UPSEuG\nXrqyRwtZdS0r15DOj4uSET+EdDBF01b5ioVMh9QfbNibYdv0r/s5Jvz5O2xyjOZbL9sj+Hpn\nv4y3ar3MHLtzLLJl+47z7+3V0sMBS0JauLJHC1l3NRujejyEdDkUW8hD6veR6sch1eb54LvN\n2Lv+AOxoYykTPtiNxluzXuaOPdgv+tky/95eLn3MWBDS0pU9WsjK61nQh5RNhdQ9bmSHNQt5\nGXX0hdd0ly1gYuz9cD95/E2+qNDpaY/HW7Ne5o59WcCiow3fhn75CxdDL17ZD0mEdDkKUz8/\ncfE0bLXsAeP9qKMvvB4qXfZdTIz9/Bn19S5f+Czb1LRfxlu4XhaNvWyVTA89Gmz583azh16+\nsh+SCOkw/BA/Pe/eXoe9PKdxXPGs6cuooy9cLtb9IYbLAuplC5gY+ymk0/JjSFPTHo23ar0s\nGrteFOnk0O3oXrL86Ya5Q69Y2Q9JhPTmzIb7sMOz7Oddf0TAdtTRF7o7yXBewF+/gHI42LBo\nw/3bmQ3X2dcrbtrpad/HW7VelozdFIvu8d/WyP1eUiw+SWXm0GtW9kMSIbW7xxHo+2jXC83l\nvK81x2JeRh194XC/eF3AwieSpsZ+fHE/3soTDD0ab916mTf29Vy7ZffLL2vkvg52Cw9+zx56\n1cq+SyOkZjib9zH6+ELdraBi1XMDL6OOvtCe8tvF/qu7pZsbk2O3j/Wz4radGHo83qr1MnPs\ndsUq+bJGXm5X+dCrVvbdBiEB6SEkQICQAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUIC\nBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQI\nCRAgJECAkAABQgIECAkQICRAgJAAAUICBAgJECCk+JSZyet2+DTucvxJ6NgQIUUn7z/CPmv6\nkA79xXzrCaElpPj8mbxp96bsQ8qqtsrM39ZTAiHFpzDntm1M1od06v58MsXWUwIhxceY/y4Z\nbsMAcCPEhpCCxI0QG0IKEjdCbPLRPlJ3qdtH2m89JRBSfI79UbtyfNTutPWUQEgRGj2PNFzk\noF0ICCk+ZRfP9cyGwuyOW08HPUKKF0cZAsJtES9CCgi3RbwIKSDcFvEipIBwWwAChAQIEBIg\nQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBI\ngAAhAQKEBAgQEiBASIAAIQEChAQIEBIg8A81BpKbde9v5QAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "printcp(model) # display the results\n",
    "plotcp(model) # visualize cross-validation results\n",
    "summary(model) # detailed summary of splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAgAElEQVR4nO2diXbbuA5AmW4z077G//+3L7EtCaREWwsEA8i956SxFpMEwGst\nttNyAYDDlFcPACADiASgACIBKIBIAAogEoACiASgACIBKIBIAAogEoACiASgACIBKIBIAAog\nEoACiASgACIBKIBIAAogEoACiASgACIBKIBIAAogEoACiASgACIBKIBIAAogEoACiASgACIB\nKIBIAAogEoACiASgACIBKIBIAAogEoACiASgACIBKIBIAAogEoACiASgACIBKIBIAAogEoAC\niLSePz+/lfL247/bUilbcjfs/c9bKT9XPfnn1l6KZMPQ+g39+HfT834e6TQ6iLSaH8P8evvz\nubhLpH8+n79GpD9vRT5vZRfKIpXyff2zhhF/Ub508Jv4LubX/y47Rfp2e+7zJw87vFiksv6Y\ndLDT6Hzp4LfwcTx6+/f9cvn77+c5z2XnxFn9pL3TUmM6D228/yrlm2nPgfnSwW/gfx8e/b0/\n/PbP56Nh4vz3ecr37dd14/s/n8et+1VUtXDfuz3QvP/6OEZ9/31pWpoOK+P0/P3z86Twvufn\n2t8frf/8XztOMZ0/Hv79Vn5de3krb7/uw68WFpnaEKP+92OgP/+2Ax17UTgQxuYLh76JX7Oz\nnPu0Gc/4Pi6c/r6JS4tq4bb3zI9hn19NS3ORho0/hr5/TSeZC6O6P/x27X3o5Y/s8k8/1iWR\nbheIt9cSGfLQCyK9egBB+JgpzYv4bdp8nOh9f7969jHFPw4aH8ef9+9X6aqFjkiDa+V33dJs\nx/FGx82kcam0d8pqkcp1DEMvb7LLt36sQxt/f866+9aGPPSCSK8eQBDmk+S25ttdsEGUz4X3\n64SrFpZP7T6m5Nv/rq59a1pqdvz98evjAu39n5tzn2vffl9dXR7V8PBzwt8n/vtN7GqhH+vI\ncIPy807l77dr77OBXnvhGgnW8HDKDkufr/bDZUy9sCzSj5sW77eLrmrPesefw4nlr9sx6O7T\n+zORrt1/9HKf6T+ahX6s4kh5W3HV7nd1BBwH+lssflm+dPAbeDBl//736/vttOaf++nWdWZV\nC8sitY1OLc13fL/tsbS1M84yOjPw1ixMO1anZeMu/7w3jQ538eqBvnfG8qX40sFv4Nvsuv4+\ncf77VqaJONy8ul6UVwsrRKpa6u24TaTh99RwtTDt2C5/uPJ9uk8pRCq9gSISrKF31+7zKvvb\nz3//d59G7//d7l99bxeei1S31D8ivc22zkdVPXyTO70tTPdFka73V76PK8RRZ3mgiARr+DO9\nPv+R7yN9u18iiGn0++fCwqJI3+U1Ut1SveOP+TXSRTY3sSDSj+EqZrbQQRg4vgl27f739cpq\neaCIBKsYP9nwee0zfbLhPn1uL8/fxquSt2ZhxV27qqXr0vu448Jdu4tsbmJBpP9u99z+ux5g\nqoUOwxP/DBdSn8er/2537f5dGuj4rPetWc0DIq3kfXzPp7rq+X59N/VzipXrfP/+93qb4Vez\nsCzS9D7Sv01L1y3iedMH/X4uNCNYEGnq5U+7sMz4xOFAKCJvQ657+bUnszlApLW8j5P5m3gb\n5c8ww64zc7i/cH21rxaWRfojPtlQt/SzVNdWo0n1lyvWifS7TL3UC8uMT/y8R3g/LP6aXkHq\ngY47DyP+oiDSen7/fPu8zBafd/vgfx8T6O3n//7ezveul0Tf/x32nxaWRbp+8K38uLVYt/Sj\nvhq6dd70vVKk2yf67r3UC4tMT5w+vfB5q+7t1/t8oNPOP+afs/hCIBI8Ze4rtJAheAoiPYcM\nwVMQ6TlkCJ6CSM8hQ/AURHoOGQJQAJEAFEAkAAUQCUABRAJQAJEAFEAkAAUQCUABRAJQAJEA\nFEAkAAUQCUABRAJQAJEAFEAkAAUQCUABRAJQAJEAFEAkAAUQCUABRAJQAJEAFEAkAAUQCUAB\nRAJQAJEAFEAkAAUQCUABRAJQAJEAFEAkAAUQCUABRAJQAJEAFEAkAAUQCUABRAJQAJEAFEAk\nAAUQCUABRDqN5dSWgZX779mran1ckGvLNJRVvcIzyONpdETauP+2PcbdymxBri3jGlCCZJ7G\nq0Qqct9xoVSLiKQNyTyB2wlTGU+myrRCJHya2sPmetdyaRq4/ltED6KLqcHnIpXmoAUKkE19\n7rN0tGOcvc30LfXPfNdZA+LxpdpDXgGtFokrJEXIpDrVhC7tXJb3Gkp94TLbdWmtUEda1fj5\nSKRWT9CARKrTinT9VcrCzJ2JtLBru3ZRpKqKT0RqRsEE0IE8qjMTaZBgfggo7f7zXdu1R49I\n1dZ2PLAb8qjOk1O7ak+tU7vL6muk9m0sJoAO5FGds0QqD0W6zN25zHatj37VGjgGedTnPksr\nD5aukUr9s7zrtFZoVxk1K6G0c/6ccW29Bo5BIk9Avi90WyxSiPu51XREmPavd60bKJcyfx/p\nslTC8e0nubDwlhO3v/Ugk1Fg0ruG6kQBkVxDdaKASK6hOlFAJNdQnSggkmuyV6eAQ149K04g\nY0ySPPElmn55IpnIGJMkUnzVF5iqFffX8VK9KVSmPYK9yIca7EoyxiSJFF/1/aMiVgwfeKjf\n571ukbuFIdJY15IxJkmk+JrP1s0/ZDT/4Nyjzzf4JdJY15IxJkmk+NaLdFtGJEdkjEkSKb6u\nSNM1kljBEckVGWOSRIqvL9L1N6d2nskYkyRSfIgUmIwxSSLFN53JiZ/Fu3bDCkRyQ8aYJJHi\nE1Y0XzeavY80XDIhkhcyxiRJFV+WYLLEIckYkyRVfFmCyRKHJGNMklTxZQkmSxySjDFJsscX\nkoxFyRiTJHt8IclYlIwxSbLHF5KMRckYkyR7fCHJWJSMMUlixSc/jyoeiO+VloXNF76P9Hoy\nxiQJFZ/4u47VpxzGtePnHEp/vwBEGutaMsYkiRSf/B8pJzfGz9rd/3n0gbsgRBrrWjLGJAkU\nX5GjnYnU8af2LAiRxrqWjDFJYsU3ncdtFSnURVKksa4lY0ySWPE19xgu7b89kbjZ8HIyxiSJ\nFV99ane/RSe2VGs5tfNExpgkseIrzcPS/KGG+7/iKxTtXb0QBBrqajLGJIkVXyvS9KCxBpG8\nkTEmSaz46kuj0q6r7+V1bt8FINBQV5MxJkms+O7ncxehz8KZXbuZPxDpgIwxSWLFV9+1kw+e\nbOau3avJGJMke3whyViUjDFJsscXkoxFyRiTJHt8IclYlIwxSbLHF5KMRckYkyR7fCHJWJSM\nMUmyxxeSjEXJGJMke3whyViUjDFJsscXkoxFyRiTJHt8IclYlIwxSbLHF5KMRckYkyR7fCHJ\nWJSMMUmyxxeSjEXJGJMke3whyViUjDFJsscXkoxFyRiTJHt8IclYlIwxSbLHF5KMRckYkyR7\nfCHJWJSMMUmyxxeSjEXJGJMke3whyViUjDFJsscXkoxFyRiTJHt8IclYlIwxSbLHF5KMRckY\nkyR7fCHJWJSMMUmyxxeSjEXJGJMke3whyViUjDFJsscXkoxFyRiTJHt8IclYlIwxSbLHF5KM\nRckYkyR7fCHJWJSMMUmyxxeSjEXJGJMke3whyViUjDFJsscXkoxFyRiTJHt8IclYlIwxSbLH\nF5KMRckYkyR7fCHJWJSMMUmyxxeSjEXJGJMke3whyViUjDFJsscXkoxFyRiTJHt8IclYlIwx\nSbLHF5KMRckYkyR7fCHJWJSMMUmyxxeSjEXJGJMke3whyViUjDFJsscXkoxFyRiTJHt8IclY\nlIwxSbLHF5KMRckYkyR7fCHJWJSMMUmyxxeSjEXJGJMke3whyViUjDFJsscXkoxFyRiTJHt8\nIclYlIwxSbLHF5KMRckYkyR7fCHJWJSMMUmyxxeSjEXJGJMke3whyViUjDFJsscXkoxFyRiT\nJHt8IclYlIwxSbLHF5KMRckYkyR7fCHJWJSMMUmyxxeSjEXJGJMke3whyViUjDFJsscXkoxF\nyRiTJHt8IclYlIwxSbLHF5KMRckYkyR7fCHJWJSMMUmyxxeSjEXJGJMke3whyViUjDFJmvhK\nKQsLw4NyxWhk4VlOVFlKYqn3Xnrm48r4L5HHMWlSZktltlA9gLV0RFra+FykFZXxXSK3A1Oi\nzBdKszAVKHsydHko0izxj0V6UJnqgd8SuR3YAuVyPajfD+3l9kAc8y9Vxotwo63QbMF3kbwx\n5Fqeb91X1IcVUYXS7ij2Wi+S3xL5HdmcSY0yLI323H+qjA/rmqot1u7unsvTb2+MeZ0e1yvk\nXrIy1Y6X55Up84bclsjnqJapDzHVGVkr0LRcXcOKf+WCPHpFSshrqCb/7HAhRZJbFg4wTysT\nqUQuB9Vhl0izBvqndu0jWKQV6fqrrBCps+O08PyIVI/AEx7H1GN4aSpNNZqUl2qvNUek5Qew\nzEykoRxPRap23HJE6tXKER7H1GNWqAciib3WXCM5r5IrtE7tnlcGkc5hzaldJVJbgm3lgmXW\nidRWYUmky+bK+C2RxzH1kLY0ItV1m62TLZR2oVStRUrIi7inqRKpvfSZVWG242KbzcKDB95w\nOagOQ43aGzhFrF3cSzRRpgo/+CAKPKQ0rz3l9v7RkFRxi7p+H6nacbHN55XxWiKfo9Ije3wh\nyViUjDFJsscXkoxFyRiTJHt8IclYlIwxSbLHF5KMRckYkyR7fCHJWJQIMZVX8erAX8bLMq7D\nS1L2ik438qoxRsjNOcSOHJE6PBxj9a2YasX9talU70GINU9fuiLk5hyUIj9cgZ3dntGoy043\n8lgk+RkG+UGF8Z3A2ccg6m8y7ew3NTqRH6/AgX6tiTBZnhyRLpM3s0+uXNoNcg9E6qEo0uVI\nBY71a0qEyaIk0vXX7AsYe/tNzUkiXX9tqcCxfk2JMFn2iTSdj4sVHJFWoX2NNLS5tQI7uz2h\nTZ+dbmSnSIsbEGkNipEfqsDuLu2JMFkQyRpEitHpRlaJVP1U10r1BkRageI10qEKHOjXmgiT\nZZ1Is+8pzd7FuK5ApBVoXyPtrcDObk9o02enGzk2xv3PjpCbc9CN3DqPiNQBkaxBpBidbgSR\nrEGkGJ1u5FVjjJCbc4gdOSJ1QCRrYkeOSB0QyZrYkSNSB0SyJnbkiNThyRjvm+V3XtoHzX6l\n3ryz38SoRN7WQXx9tVR/nF18qVWnY41GQnS6kSdvm06alOUH0zvqy5v39ZsZjciXE12/prX9\n6XzRD5E6PPlkg3iHvCw6VLr7rfjQxJdEIfKl1zK5uupmFAuRzuTRGKvPm1QVqkUqVTOI9Bit\nyGdH/YcHpKLUMSJ1WHMp01wGXVqRLoi0AaXI27/YgEivZaNI8sOryyLNzzj29JsYzZsNZV6C\nJvdiLSKdyeYjUm3K7IxcnL8j0iL6p3bzfxdvQCDSmWwXqTp3617alicXtxFycw5qkZfVIq04\nQ9jQqT0RJouySLI5RFrkFSLp/ZVUROqwSaTegyfn6Tv6TYxC5PM6PLlE0uoYkbqsEWk8HZ/9\nR5ZLp+LyCbv7TYyGSG0dmn8RyZ5VInU/IrR4BlHtt7PfxOjM5ycf1VrqDJHO5FVjjJCbc4gd\nOSJ1QCRrYkeOSB0QyZrYkSNSB0SyJnbkiNQBkayJHTkidUAka2JHjkgdEMma2JEjUgdEsiZ2\n5IjUAZGsiR05InVAJGtiR45IHRDJmtiRI1IHRLImduSI1AGRrIkdOSJ1QCRrYkeOSB0QyZrY\nkSNSB0SyJnbkiNQBkayJHTkidUAka2JHjkgdEMma2JEjUgdEsiZ25IjUAZGsiR05InVAJGti\nR45IHRDJmtiRI1IHRLImduSI1AGRrIkdOSJ1QCRrYkeOSB0QyZrYkSNSB0SyJnbkiNQBkayJ\nHTkidUAka2JHjkgdEMma2JEjUgdEsiZ25IjUAZGsiR05InVAJGtiR45IHRDJmtiRI1IHRLIm\nduSI1AGRrIkdOSJ1QCRrYkeOSB0QyZrYkSNSB0SyJnbkiNQBkayJHTkidUAka2JHjkgdEMma\n2JEjUgdEsiZ25IjUAZGsiR05InVAJGtiR45IHRDJmtiRI1IHRLImduSI1AGRrIkdOSJ1QCRr\nYkeOSB0QyZrYkSNSB0SyJnbkiNQBkayJHTkidUAka2JHjkgdEMma2JEjUgdEsiZ25IjUAZGs\niR05InVAJGtiR45IHRDJmtiRI1IHRLImduSI1AGRrIkdOSJ1QCRrYkeOSB0QyZrYkSNSB0Sy\nJnbkiNQBkayJHTkidUAka2JHjkgdEMka35E/G911eylit/tCufGSQXkAkazxHfkakYrcb7bw\ngkGd3dPii0Sp9156ZvOc9gWnVK9BqiP+CjzN+Lhw1gv8I1aIVOSOpXpSVpGWNj4XqdSrRTVL\ntflQeIjUrJq/xJflnc+l3PUt4ylbu70WqXp81midiFRvfSpSk6dyqUUqzaOdIFK7pjQL8/lq\nwd2i8fd0XBzPSWYDKxlEal4/hvOuzxXNEXd6hSntjtVw6zxJ72TujkaXSKR7Csf83mbbVI9L\n9epT6ixeLk3GXy3SUu/1SUn7Snu6RxZJaF8/xjwsni0Me7c7XubpmJd7JtKhmzSZRKpOxMr0\nz6XK+PDgScbrI755mhZFqh/MDS+X2Sr9QZkwC3+s7WVhy2zHy/xlckGkZlVp1u0YcQ5EIqrM\nt2kef42qyQYWDk8vu9mw6Yg0LSQS6fqrlEszy+cidXacFp6LNFu5fcQ52CNSP+PujkjPr5Ey\nijRE+lSkasetR6RFpXaMOAfD0aM0r0z1dByyPlybPj0inTw5Oyy+BNTbuwfQcwdlwnL4D0Sa\n5WnFNVJpV7SPN484B7NcPxDp8jTjXkTqHA/nUVzOH6oXkapYOyJd5pVcXP9gv+0jzoHI9dMj\n0vhoeqZTkTpXaMPhd74QW6R7darCtZc+spqVSNMJ71Kb1bJYVXr7bR54Emb5Ff+0r9fDa/28\nhXlaD+X3KL2OXzIgi07F+xTltliEKPIWdf0+UrXjYputQWXcKD4jtH/cu5/pjikx45L4p8zS\nt9TE8kv8C+7ajSPauP5UIkyWV40xQm7OIUbkiLQRRLImduSI1AGRrIkdOSJ1QCRrYkeOSB0Q\nyZrYkX85kYo22uNTbs8C9ZwewXtsqoPTbOzFfUec+Np4yoH3+uYTqfqqUrXi/rpRqjcuxJrq\nZWVvLJs6cc7KkSplXGUsq2nbOzrUhCLJN9iLWDG+PVu9ozutqd9Y3xnLtk6cs1YknYyrjGU1\nZWHxyFATinT/p1zkgzLtUW+Qe2iJtLoT56w+Il00Mq4yltUsiXQ5MNSvLNL11+ybAG17+4ax\nrhPnqIt0/bUvGdYiXX9tGOoXEmk66RUrtI9I2zpxzlGRNJNxskiHh/qVRFrcoC3Spk6cc1ik\nxQ0+Rbqv2z1UREKkLoiESM2PzEjzF4h0RdrWiXM2inQw4ypjWU1ZWDwy1MQizb4wM3ur4LpC\n+Yi0qRPnbBXpWMZVxrKatr2jQ80nktKeB2MJI8sjtILQaMfi1O5IN4h0uL0znu4DRDrcnoPG\nTusbkdaCSIfbc9DYi/tOYcJBPOXAe30Ryai9iHjKgff6IpJRexHxlAPv9UUko/Yi4ikH3uub\nVqT7svgaSbumVAvt100OxzLrV/6K8a2kLWNsEy2+NyreUXqUcb2x7GpPjFfMkPnazvdhs4ok\n/gBh6awpYqHMWjgai/g7lbLB0unOJRuGOE/01MBiorem4HyR5INq4PVa7dt9pzd2qO8igy6L\na8osXaVu4eBwSv2giKVHBfHE+iFWuS2z1fNEL2Rcayw72yvV7yJ+5mvXtKc6OENaCzraPBKp\n396e0SzNmfYlzjkbh9gJUS72NqmPZXN7CyNbHG9vHDlFukxVnTbNRRI/p18jtbMswkXStiE2\n/2vLpUn/PXr5cTxXIpXZyMrS2t5fDPraIk1/pGPWwhkiTb3kv9lwqfN+/12aNJwzlj3tjbNj\n+SymPiKpn8Gc2djBvjsijeWsUrRQVmWRyqWtQQCT9pzazX5XATcvKo5EGtcunMSVxVjWtKc6\nOBvWinQRn45vcnaySG0n/k3aOsKFQ1CzJFbd63DWWPa1h0jrRbo/fIFIzZ/k/FoilfkqV6d2\n41pEWliel7JKk61I09JCdy5ZP8LFQ39303LGtcayr735yJZnSG/guUUSyWjWNJvlfsvt7R9Q\n3fZ1ad6dSzaINE90/xjUybjWWHa2146sN0M6A08uUvcjQu3mk29/l2b1V7hrJxSSnxfy+hGh\ndmTdGfLFbn97ay8innLgvb6IZNReRDzlwHt9EcmovYh4yoH3+iKSUXsR8ZQD7/VFJKP2IuIp\nB97ri0hG7UXEUw681xeRjNqLiKcceK8vIhm1FxFPOfBeX0Qyai8innLgvb6IZNReRDzlwHt9\nEcmovYh4yoH3+iKSUXsR8ZQD7/VFJKP2IuIpB97ri0hG7UXEUw681xeRjNqLiKcceK8vIhm1\nFxFPOfBeX0Qyai8innLgvb6IZNReRDzlwHt9EcmovYh4yoH3+iKSUXsR8ZQD7/VFJKP2IuIp\nB97ri0hG7UXEUw681xeRjNqLiKcceK8vIhm1FxFPOfBeX0Qyai8innLgvb6IZNReRDzlwHt9\nEcmovYh4yoH3+iKSUXsR8ZQD7/VFJKP2IuIpB97ri0hG7UXEUw681xeRjNqLiKcceK8vIhm1\nFxFPOfBeX0Qyai8innLgvb6IZNReRDzlwHt9EcmovYh4yoH3+iKSUXsR8ZQD7/VFJKP2IuIp\nB97ri0hG7UXEUw681xeRjNqLiKcceK8vIhm1FxFPOfBeX0Qyai8innLgvb6IZNReRDzlwHt9\nEcmovYh4yoH3+iKSUXsR8ZQD7/VFJKP2IuIpB97ri0hG7UXEUw681xeRjNqLiKcceK8vIhm1\nFxFPOfBeX0Qyai8innLgvb6IZNReRDzlwHt9EcmovYh4yoH3+iKSUXsR8ZQD7/VFJKP2IuIp\nB97ri0hG7UXEUw681xeRjNqLiKcceK8vIhm1FxFPOfBeX0Qyai8innLgvb6IZNReRDzlwHt9\nEcmovYh4yoH3+iKSUXsR8ZQD7/VFJKP2IuIpB97ri0hG7UXEUw681xeRjNqLiKcceK8vIhm1\nFxFPOfBeX0Qyai8innLgvb6IZNReRDzlwHt9EcmovYh4yoH3+iKSUXsR8ZQD7/VFJKP2nFIe\nxSm3lWZP6/x4ry8iGbXnk8dRlvph7dUp41k1lvztIVI01opUZsuIdGJ7iBSM8nnCVm6Hl/Hc\nbTqJK2MaGpGKfX681xeRjNrzyc2g8VH1oFHqgkhm7SFSNCp9hCDVIerSitReMFngvb6IZNSe\nTx6IVB11KpHEE+zwXl9EMmrPJ5NIN8SD7hGpvfFgg/f6IpJRez5pjkiXOu7la6QyuWaI9/oi\nklF7Pnl0jfTgrt2FI9Kp7SFSNKbbBot37ZZ2vMy2meC9vohk1J5Plu51tyd09dqlbRZ4ry8i\nGbUXEU858F5fRDJqLyKecuC9vohk1F5EPOXAe30Ryai9iHjKgff6IpJRexHxlAPv9UUko/Ze\nRXkV2nF8pfayTL5MvKomzIUDGCWvDB/8Hx7cfpfqrRCx5uHL46EnB+Dh6KeoVXP6tN9Hz9Oq\nx0mxmcwXmyk3vIl4H/74fvyw3K6ZvSnfNrb7yRF4LNKYy4tiTp/3++RpKvU4Jzab+WIy48rw\nb5G/ZxvkHs8Ss/fJEXhyRLosB308LUdEOtRx1ZJ6bDbzxYlI119lZ2K2PDkCSiJdf21Ji5ZI\nmzuuWlKPzWa+vEak6SRVrFj9InHoyQHYJ9LxtOxNmlo9TorNZL68SCS5vOtoe+jJztkp0uIG\nC5EOd1wN4LTYzp0viOQPRJo3cLSLHCJdexl+ZASl3rAutkNPjsAqkXRz+rzf5yM6Xo9zYrOZ\nL0Yzbnxf4P7P7Nb+dcXK2A49OQDrRNLN6fOtD56nVY+TYjOZL6+dcYd6Dy3LI44Ftv/ZBxNq\nUA/H8wWR/IFIp3SBSGc82TOIdEoXmUWCJV5VE+bCAUiePxApICTPH4gUEJLnD0QKiEXy7n3I\nb4O0D5q1K0dVmgfiHYd6eyyeD3seY5XKByk/1u+6Bg4MYdZi9dbP0OzmpmQz4xhLGVbvmHrz\nLnY+b0sXIgPLD4Y3x5q1KxueHpT62VG/3/d02CLINrnT2/aXzTk9mq7K5X1DWBjRg6lyvBmF\nMYpGzmYcpBi/fNBZ+7zZUj9onm3/fz0q8WzYVT7r5A6LnZQf6/fp06XKS0PY2sGDqbKlqeVm\nptfb+eYd+BJpy7Ae2Feq1eFYNewqxlKtnq1dmYdj6Xow3+spu6Ph6cH+prozbmnybOeFIlWH\n1FKf+K4b1sNc5BapVDNKfIxsaW0xvUaqhyDX6Im0o6nZjBuvkOa9HBnliXTPM0T162/Ob30J\nXXjhq9qLxfNhtzcbmj9H0K5d+WcJzhVp73X87OVxX1P3kYgZN6zVOGr6OCLtjmYhx7LtvCJd\nmlQ1yX3w2nW436cNLA9hfwdLB6QdTVXeFLk6vEjVg/mL2dqGmx6WvAzGumEvvvD7FmlPD50X\ny61NLadpnp3dOfAo0uoxLSZ5OPiXA28KvBZEWhhTG8O2pprp1RVpfwocirR+SEsilaXtsXg2\n7PlLqH+Rds7TVsGdZxvV4aadcbumXn+kJzKNtCw9WNi8qeFLN79ZRSpipyp7C6/fdcoP9buq\ngc4Q9v0ZxiWPdjTVzpNHE3IvdiJ1PyJUr91yTjYXqX52UpEus7t2TSofpPxYvysaODaEpr2h\nmsODfU2JSbE043ZNvXkn+54GJ/KqmjAXDkDy/IFIASF5/kCkgJA8fyBSQEiePxApICTPH4gU\nEJLnD0QKCMnzByIFhOT5A5ECQvL8gUgBIXn+QKSAkDx/IFJASJ4/ECkgJM8fiBQQkucPRAoI\nyfMHIgWE5PkDkQJC8vyBSAEhef5ApICQPH8gUkBInj8QKSAkzx+IFBCS5w9ECgjJ8wciBUwP\nnI4AAAlISURBVITk+QORAkLy/IFIASF5/kCkgJA8fyBSQEiePxApICTPH4gUEJLnD0QKCMnz\nByIFhOT5A5ECQvL8gUgBIXn+QKSAkDx/IFJASJ4/ECkgJM8fiBQQkucPRAoIyfMHIgWE5PkD\nkQJC8vyBSAEhef5ApICQPH8gUkBInj8QKSAkzx+IFBCS5w9ECgjJ8wciBYTk+QORAkLy/IFI\nASF5/kCkgJA8fyBSQEiePxApICTPH4gUEJLnD0QKCMnzByIFhOT5A5ECQvL8gUgBIXn+QKSA\nkDx/IFJASJ4/ECkgJM8fiBQQkucPRAoIyfMHIgWE5PkDkQJC8vyBSAEhebbM8l3m6/s1KaUs\nLMwfKI0N1kPy7CnzhSJXdmtS5MZxYf5AY1ywDZJnz16Ritw4PG/hgcq4YBsk7xyGs6xye1CG\nFWVaNWy5rRaWFPH8y7S6flzqFUWhlMyFA5C8Uyji5ybPwspp+ldbhrWLFz5lEml0dfzhGul1\nkLwzeHz+NVOmucwpF2FIfR44LcrD2f1gxzXSCyF5Z1AfQ5ZEqrd0RZqXZ+bo3E2NUcNGSN4Z\niONGWT4iVVvKjdsu5XaONl5JddrmZoMvSN4ZbDy1EwrcRZJtNS4hkktI3hnoiFSa5sriAiK5\ngOSdwl2P8SbA7K7dtKXRrjR6yUartfV1VfXgwKBhJyTvHMY7abdbB/J9pPvl0bhlSaTOnWx5\nx5uPCLmC5JmwKc2vqglz4QAkzwREyg7JMwGRskPy/IFIASF5/kCkgJA8fyBSQEieHUUb7fEp\nt/elIHl2MPETQzHsEB9KEN/7q1a03wNs13S/77dpHFs6gXWQMjsmkWYfGKo+UiQ/DFSvqT8E\ntLN22zqBdZAxO+oPoc4+b9p+WGhxDy2RVncC6yBjduwQ6fqrnCzSw05gHWTMjucitX8RZZrg\niiJt6wTWQcbsWCHS4gZtkTZ1AusgY3YgUmLImB0zkaqf6lqp3nDCNdLqTmAdZMyOuUjy6371\nNZJ4o6lcdEXa1gmsg4zZsT7X6/Y8WDtKrwnZtAOREkM27UCkxJBNO7RzTe0cQTHsQKTEUAw7\nECkxFMMOREoMxbCjznUR7+IMy/JtpWF38e2gpliHa9f2UMTCKd/BTQypsqO0C9UnHMQu40cN\n6s3tvD5au1kPpRkWrId82VFmj8ulPvpUny1oD06lLdbh299ND6VZgA2QLzvmuW5Fmk/qhWn+\noL2No3nQA/NiIyTMji0iXWbTfN6AzjVSI5K8Yjva/leCZNkxy3V1dXJpfXmJSOUihsXkWA+5\nsmNRpPutMrHD/HrpVJHGAbRaa/TwdSBVdrS5FnefXynSRXyJApH2QqrseODBS0W6P0SkI5Aq\nO8rSUnV/u9rN+hqptAvMjg2QKjvK4kLnjkNzF2DegO410qw72S88h1zZUZ3LiQ8EyU/ozM+r\nrD4i1HbH7e9NkCw7tHNN7RxBMexApMRQDDsQKTEUww5ESgzFsAOREkMx7ECkxFAMOxApMRTD\nDkRKDMWwA5ESQzHsQKTEUAw7ECkxFMMOREoMxbADkRJDMexApMRQDDsQKTEUww5ESgzFsAOR\nEkMx7ECkxFAMOxApMRTDDkRKDMWwA5ESQzHsQKTEUAw7ECkxFMMOREoMxbADkRJDMexApMRQ\nDDsQKTEUww5ESgzFsAOREkMx7ECkxFAMOxApMRTDDkRKDMWwA5ESQzHsQKTEUAw7ECkxFMMO\nREoMxbADkRJDMexApMRQDDsQKTEUww5ESgzFsAOREkMx7ECkxFAMOxApMRTDDkRKDMWwA5ES\nQzHsQKTEUAw7ECkxFMMOREoMxbADkRJDMexApMRQDDsQKTEUww5ESgzFsAOREkMx7ECkxFAM\nOxApMRTDDkRKDMWwA5ESQzHsQKTEUAw7ECkxFMMOREoMxbADkRJDMexApMRQDDsQKTEUww5E\nSgzFsAOREkMx7ECkxFAMOxApMRTDDkRKDMWwA5ESQzHsQKTEUAw7ECkxFMMOREoMxbADkRJD\nMexApMRQDDsQKTEUww5ESgzFsAOREkMx7ECkxFAMOxApMRTDDkRKDMWwA5ESQzHsQKTEUAw7\nECkxFMMOREoMxbADkRJDMexApMRQDAAFEEmRcinlltD773FFuf2edmjWjKsXmz3yZLCBEihy\nn93i5/7gbtW0Q7NmXL3Y6pEngxFUQJFxgl/kgzJlud4g93gm0t4ngxFUQJH1Il1/lZ0ibXky\nGEEFFOmKNF3UiBWrDyqHngw2UAFF+iItbtjiwqEnw/lQAUUQ6etCBRSZzuTEz+Jdu9uKDddI\ne58MRlABRcTELvUkn70VdF2x9Rppz5PBBipgyaFsUyrPUB1LECktVMcSREoL1QFQAJEAFEAk\nAAUQCUABRDqD4Z2fG8PK4d9p7aZvEhXxNpJscXzniu8lvRBSfwLTR0zFgzKJtLjf6kanZw1e\nyW7gNZB/feQsF4eNxoJ2v7WNth9nKBdEcgD5P4vZ6Vf9kdbLrtxXDsoGqeOLoQBn0RdpvELa\n/tcWhtbmInGF9FpI/knM7we0HwXf8dcW6psNpfmhmC+E3AMogEgACiASgAKIBKAAIgEogEgA\nCiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAA\nIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiAS\ngAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEo\ngEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKI\nBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgA\nCiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAA\nIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiAS\ngAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEogEgACiASgAKIBKAAIgEo\ngEgACvwf+unllJx542EAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Classification Tree - Rpart\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot tree\n",
    "plot(model, uniform=TRUE,\n",
    "   main=\"Classification Tree - Rpart\")\n",
    "text(model, use.n=TRUE, all=TRUE, cex=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 26 √ó 1</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>model.variable.importance</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>charExclamation</th><td>369.7542255</td></tr>\n",
       "\t<tr><th scope=row>charDollar</th><td>276.1117487</td></tr>\n",
       "\t<tr><th scope=row>capitalLong</th><td>202.1765364</td></tr>\n",
       "\t<tr><th scope=row>free</th><td>166.8168531</td></tr>\n",
       "\t<tr><th scope=row>your</th><td>163.3270199</td></tr>\n",
       "\t<tr><th scope=row>you</th><td>135.4577268</td></tr>\n",
       "\t<tr><th scope=row>remove</th><td>124.5453816</td></tr>\n",
       "\t<tr><th scope=row>num000</th><td> 59.9175289</td></tr>\n",
       "\t<tr><th scope=row>capitalTotal</th><td> 53.6622872</td></tr>\n",
       "\t<tr><th scope=row>money</th><td> 49.1334615</td></tr>\n",
       "\t<tr><th scope=row>make</th><td> 43.2221535</td></tr>\n",
       "\t<tr><th scope=row>hp</th><td> 32.6694816</td></tr>\n",
       "\t<tr><th scope=row>capitalAve</th><td> 31.4224397</td></tr>\n",
       "\t<tr><th scope=row>hpl</th><td> 20.9084682</td></tr>\n",
       "\t<tr><th scope=row>telnet</th><td> 15.6813511</td></tr>\n",
       "\t<tr><th scope=row>labs</th><td> 11.7610134</td></tr>\n",
       "\t<tr><th scope=row>num650</th><td> 11.7610134</td></tr>\n",
       "\t<tr><th scope=row>charHash</th><td>  8.3153398</td></tr>\n",
       "\t<tr><th scope=row>george</th><td>  6.5338963</td></tr>\n",
       "\t<tr><th scope=row>font</th><td>  5.2198016</td></tr>\n",
       "\t<tr><th scope=row>receive</th><td>  4.3256302</td></tr>\n",
       "\t<tr><th scope=row>num3d</th><td>  4.2347833</td></tr>\n",
       "\t<tr><th scope=row>charSemicolon</th><td>  2.0076160</td></tr>\n",
       "\t<tr><th scope=row>order</th><td>  1.2045696</td></tr>\n",
       "\t<tr><th scope=row>email</th><td>  0.4466226</td></tr>\n",
       "\t<tr><th scope=row>our</th><td>  0.4466226</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 26 √ó 1\n",
       "\\begin{tabular}{r|l}\n",
       "  & model.variable.importance\\\\\n",
       "  & <dbl>\\\\\n",
       "\\hline\n",
       "\tcharExclamation & 369.7542255\\\\\n",
       "\tcharDollar & 276.1117487\\\\\n",
       "\tcapitalLong & 202.1765364\\\\\n",
       "\tfree & 166.8168531\\\\\n",
       "\tyour & 163.3270199\\\\\n",
       "\tyou & 135.4577268\\\\\n",
       "\tremove & 124.5453816\\\\\n",
       "\tnum000 &  59.9175289\\\\\n",
       "\tcapitalTotal &  53.6622872\\\\\n",
       "\tmoney &  49.1334615\\\\\n",
       "\tmake &  43.2221535\\\\\n",
       "\thp &  32.6694816\\\\\n",
       "\tcapitalAve &  31.4224397\\\\\n",
       "\thpl &  20.9084682\\\\\n",
       "\ttelnet &  15.6813511\\\\\n",
       "\tlabs &  11.7610134\\\\\n",
       "\tnum650 &  11.7610134\\\\\n",
       "\tcharHash &   8.3153398\\\\\n",
       "\tgeorge &   6.5338963\\\\\n",
       "\tfont &   5.2198016\\\\\n",
       "\treceive &   4.3256302\\\\\n",
       "\tnum3d &   4.2347833\\\\\n",
       "\tcharSemicolon &   2.0076160\\\\\n",
       "\torder &   1.2045696\\\\\n",
       "\temail &   0.4466226\\\\\n",
       "\tour &   0.4466226\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 26 √ó 1\n",
       "\n",
       "| <!--/--> | model.variable.importance &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| charExclamation | 369.7542255 |\n",
       "| charDollar | 276.1117487 |\n",
       "| capitalLong | 202.1765364 |\n",
       "| free | 166.8168531 |\n",
       "| your | 163.3270199 |\n",
       "| you | 135.4577268 |\n",
       "| remove | 124.5453816 |\n",
       "| num000 |  59.9175289 |\n",
       "| capitalTotal |  53.6622872 |\n",
       "| money |  49.1334615 |\n",
       "| make |  43.2221535 |\n",
       "| hp |  32.6694816 |\n",
       "| capitalAve |  31.4224397 |\n",
       "| hpl |  20.9084682 |\n",
       "| telnet |  15.6813511 |\n",
       "| labs |  11.7610134 |\n",
       "| num650 |  11.7610134 |\n",
       "| charHash |   8.3153398 |\n",
       "| george |   6.5338963 |\n",
       "| font |   5.2198016 |\n",
       "| receive |   4.3256302 |\n",
       "| num3d |   4.2347833 |\n",
       "| charSemicolon |   2.0076160 |\n",
       "| order |   1.2045696 |\n",
       "| email |   0.4466226 |\n",
       "| our |   0.4466226 |\n",
       "\n"
      ],
      "text/plain": [
       "                model.variable.importance\n",
       "charExclamation 369.7542255              \n",
       "charDollar      276.1117487              \n",
       "capitalLong     202.1765364              \n",
       "free            166.8168531              \n",
       "your            163.3270199              \n",
       "you             135.4577268              \n",
       "remove          124.5453816              \n",
       "num000           59.9175289              \n",
       "capitalTotal     53.6622872              \n",
       "money            49.1334615              \n",
       "make             43.2221535              \n",
       "hp               32.6694816              \n",
       "capitalAve       31.4224397              \n",
       "hpl              20.9084682              \n",
       "telnet           15.6813511              \n",
       "labs             11.7610134              \n",
       "num650           11.7610134              \n",
       "charHash          8.3153398              \n",
       "george            6.5338963              \n",
       "font              5.2198016              \n",
       "receive           4.3256302              \n",
       "num3d             4.2347833              \n",
       "charSemicolon     2.0076160              \n",
       "order             1.2045696              \n",
       "email             0.4466226              \n",
       "our               0.4466226              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.frame(model$variable.importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred <- predict(model,test_data[,-target_variable])\n",
    "Y <- test_data[,target_variable]\n",
    "threshold <- 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 6 √ó 2 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>nonspam</th><th scope=col>spam</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>2535</th><td>0.94205607</td><td>0.05794393</td></tr>\n",
       "\t<tr><th scope=row>708</th><td>0.05283019</td><td>0.94716981</td></tr>\n",
       "\t<tr><th scope=row>4187</th><td>0.94205607</td><td>0.05794393</td></tr>\n",
       "\t<tr><th scope=row>29</th><td>0.22619048</td><td>0.77380952</td></tr>\n",
       "\t<tr><th scope=row>1333</th><td>0.94205607</td><td>0.05794393</td></tr>\n",
       "\t<tr><th scope=row>99</th><td>0.22619048</td><td>0.77380952</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 6 √ó 2 of type dbl\n",
       "\\begin{tabular}{r|ll}\n",
       "  & nonspam & spam\\\\\n",
       "\\hline\n",
       "\t2535 & 0.94205607 & 0.05794393\\\\\n",
       "\t708 & 0.05283019 & 0.94716981\\\\\n",
       "\t4187 & 0.94205607 & 0.05794393\\\\\n",
       "\t29 & 0.22619048 & 0.77380952\\\\\n",
       "\t1333 & 0.94205607 & 0.05794393\\\\\n",
       "\t99 & 0.22619048 & 0.77380952\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 6 √ó 2 of type dbl\n",
       "\n",
       "| <!--/--> | nonspam | spam |\n",
       "|---|---|---|\n",
       "| 2535 | 0.94205607 | 0.05794393 |\n",
       "| 708 | 0.05283019 | 0.94716981 |\n",
       "| 4187 | 0.94205607 | 0.05794393 |\n",
       "| 29 | 0.22619048 | 0.77380952 |\n",
       "| 1333 | 0.94205607 | 0.05794393 |\n",
       "| 99 | 0.22619048 | 0.77380952 |\n",
       "\n"
      ],
      "text/plain": [
       "     nonspam    spam      \n",
       "2535 0.94205607 0.05794393\n",
       "708  0.05283019 0.94716981\n",
       "4187 0.94205607 0.05794393\n",
       "29   0.22619048 0.77380952\n",
       "1333 0.94205607 0.05794393\n",
       "99   0.22619048 0.77380952"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>nonspam</li><li>spam</li><li>nonspam</li><li>spam</li><li>spam</li><li>spam</li></ol>\n",
       "\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<style>\n",
       "\t.list-inline {list-style: none; margin:0; padding: 0}\n",
       "\t.list-inline>li {display: inline-block}\n",
       "\t.list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "\t</style>\n",
       "\t<ol class=list-inline><li>'nonspam'</li><li>'spam'</li></ol>\n",
       "</details>"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item nonspam\n",
       "\\item spam\n",
       "\\item nonspam\n",
       "\\item spam\n",
       "\\item spam\n",
       "\\item spam\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item 'nonspam'\n",
       "\\item 'spam'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. nonspam\n",
       "2. spam\n",
       "3. nonspam\n",
       "4. spam\n",
       "5. spam\n",
       "6. spam\n",
       "\n",
       "\n",
       "\n",
       "**Levels**: 1. 'nonspam'\n",
       "2. 'spam'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] nonspam spam    nonspam spam    spam    spam   \n",
       "Levels: nonspam spam"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Y\n",
       "Y_hat     nonspam spam\n",
       "  nonspam    1285  149\n",
       "  spam        106  761"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_hat <- ifelse(Y_pred[,2] > threshold,\"spam\",\"nonspam\") \n",
    "\n",
    "confusion_matrix <- table(Y_hat,Y)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.110821382007823"
      ],
      "text/latex": [
       "0.110821382007823"
      ],
      "text/markdown": [
       "0.110821382007823"
      ],
      "text/plain": [
       "[1] 0.1108214"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute misclassification rate\n",
    "accuracy = (confusion_matrix[1,1]+confusion_matrix[2,2])/sum(confusion_matrix)\n",
    "misclassification_rate = 1 - accuracy\n",
    "misclassification_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] - Training set size: 4140 - Testing set size 461\"\n",
      "         Y\n",
      "Y_hat     nonspam spam\n",
      "  nonspam     247   31\n",
      "  spam         14  169\n",
      "[1] \"[INFO] - Misclassification rate - 1 fold: 0.0976138828633406\"\n",
      "[1] \"[INFO] - Training set size: 4140 - Testing set size 461\"\n",
      "         Y\n",
      "Y_hat     nonspam spam\n",
      "  nonspam     272   24\n",
      "  spam         22  143\n",
      "[1] \"[INFO] - Misclassification rate - 2 fold: 0.0997830802603037\"\n",
      "[1] \"[INFO] - Training set size: 4140 - Testing set size 461\"\n",
      "         Y\n",
      "Y_hat     nonspam spam\n",
      "  nonspam     254   34\n",
      "  spam         16  157\n",
      "[1] \"[INFO] - Misclassification rate - 3 fold: 0.108459869848156\"\n",
      "[1] \"[INFO] - Training set size: 4140 - Testing set size 461\"\n",
      "         Y\n",
      "Y_hat     nonspam spam\n",
      "  nonspam     277   33\n",
      "  spam         25  126\n",
      "[1] \"[INFO] - Misclassification rate - 4 fold: 0.125813449023861\"\n",
      "[1] \"[INFO] - Training set size: 4140 - Testing set size 461\"\n",
      "         Y\n",
      "Y_hat     nonspam spam\n",
      "  nonspam     270   30\n",
      "  spam         13  148\n",
      "[1] \"[INFO] - Misclassification rate - 5 fold: 0.0932754880694143\"\n",
      "[1] \"[INFO] - Training set size: 4140 - Testing set size 461\"\n",
      "         Y\n",
      "Y_hat     nonspam spam\n",
      "  nonspam     265   35\n",
      "  spam         11  150\n",
      "[1] \"[INFO] - Misclassification rate - 6 fold: 0.0997830802603037\"\n",
      "[1] \"[INFO] - Training set size: 4140 - Testing set size 461\"\n",
      "         Y\n",
      "Y_hat     nonspam spam\n",
      "  nonspam     258   27\n",
      "  spam         22  154\n",
      "[1] \"[INFO] - Misclassification rate - 7 fold: 0.106290672451193\"\n",
      "[1] \"[INFO] - Training set size: 4140 - Testing set size 461\"\n",
      "         Y\n",
      "Y_hat     nonspam spam\n",
      "  nonspam     253   36\n",
      "  spam         18  154\n",
      "[1] \"[INFO] - Misclassification rate - 8 fold: 0.117136659436009\"\n",
      "[1] \"[INFO] - Training set size: 4140 - Testing set size 461\"\n",
      "         Y\n",
      "Y_hat     nonspam spam\n",
      "  nonspam     257   26\n",
      "  spam         22  156\n",
      "[1] \"[INFO] - Misclassification rate - 9 fold: 0.10412147505423\"\n",
      "[1] \"[INFO] - Training set size: 4149 - Testing set size 452\"\n",
      "         Y\n",
      "Y_hat     nonspam spam\n",
      "  nonspam     256   40\n",
      "  spam         16  140\n",
      "[1] \"[INFO] - Misclassification rate - 10 fold: 0.123893805309734\"\n",
      "[1] \"[INFO] - Mean misclassification rate: 0.107617146257655\"\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "target_variable <- ncol(spam)\n",
    "accuracy_vec <- array(0,k)\n",
    "threshold <- 0.5\n",
    "\n",
    "# 1. Shuffle the dataset randomly.\n",
    "spam_idx <- sample(1:nrow(spam))\n",
    "\n",
    "# 2. Split the dataset into k groups\n",
    "max <- ceiling(nrow(spam)/k)\n",
    "splits <- split(spam_idx, ceiling(seq_along(spam_idx)/max))\n",
    "\n",
    "# 3. For each unique group:\n",
    "for (i in 1:k){\n",
    "    #3.1 Take the group as a hold out or test data set\n",
    "    test_data <- spam[splits[[i]],]\n",
    "    \n",
    "    #3.2 Take the remaining groups as a training data set\n",
    "    train_data <- spam[-splits[[i]],]\n",
    "    print(paste(\"[INFO] - Training set size:\",dim(train_data)[1],\"- Testing set size\",dim(test_data)[1]))\n",
    "    \n",
    "    #3.3 Fit a model on the training set and evaluate it on the test set\n",
    "    model <- rpart(type ~ ., method=\"class\",data=train_data)\n",
    "    Y_pred <- predict(model,test_data[,-target_variable])\n",
    "    Y <- test_data[,target_variable]\n",
    "    \n",
    "    #3.4 Store the prediction of the tree (2 is to take only the P(Y=\"spam\"|x))\n",
    "    Y_hat <- ifelse(Y_pred[,2] > threshold,\"spam\",\"nonspam\") \n",
    "    confusion_matrix <- table(Y_hat,Y)\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    #3.5 Retain the evaluation score and discard the model\n",
    "    accuracy_vec[i] = (confusion_matrix[1,1]+confusion_matrix[2,2])/sum(confusion_matrix)\n",
    "    misclassification_rate = 1 - accuracy_vec[i]\n",
    "    print(paste(\"[INFO] - Misclassification rate -\",i,\"fold:\",misclassification_rate))\n",
    "}\n",
    "\n",
    "#4. Summarize the skill of the model using the sample of model evaluation scores\n",
    "print(paste(\"[INFO] - Mean misclassification rate:\",1-mean(accuracy_vec)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Classification : Alternative metrics (ROC and AUC)\n",
    "\n",
    "In the previous exercise we selected an arbitrary threshold $t=0.5$ in order to determine the output classes.\n",
    "The confusion matrix and all the related performance measures depends on the choice of the aforementioned threshold. \n",
    "Usually the choice of a threshold is related to Type I error and Type II errors (cf. Section 5.13 and 10.3 of the syllabus) that we are ready to accept in a stochastic setting.\n",
    "\n",
    "In order to avoid conditioning our assessment on a specific threshold, it is interesting to assess the overall accuracy for all possible thresholds. This is possible by plotting curves, like the Receiver Operating Characteristic (ROC).\n",
    "\n",
    "The ROC curve is a plot in which, for every threshold in the range $[0,1]$, the false positive rate of the corresponding classifier (1-specificity, on the horizontal axis) against the true positive rate (i.e. sensitivity or power, on the vertical axis). In other terms, ROC visualises the probability of detection vs. the probability of false alarm. Each point on the curve correspond to a different threshold used in the classifier.\n",
    "\n",
    "The ideal ROC curve would be composed of a single point: $(FPR=0,TPR=1)$. \n",
    "The worst possible ROC curve would be represented by the bissextrix line on the FPR-TPR plane.\n",
    "In this case, for each threshold, we would have $T_P=N_P=F_P=N_N$ , i.e. the same proportion of true positive and false positive. \n",
    "In other terms, this classifier would not separate the classes at all.\n",
    "In practice, real-life classification rules produce ROC curves which lie between these two extremes. \n",
    "\n",
    "A common way to summarise a ROC curve, is to compute the area under the curve (AUC), by curve integration. \n",
    "By measuring the AUC of different classifiers, we have a compact way to compare classifiers without setting a specific threshold.\n",
    "\n",
    "For the classifier defined in Exercise 1, you need to:\n",
    "\n",
    "- Plot the ROC curve for $t \\in \\{0,0.05,0.1,\\cdots,1\\}$\n",
    "- Compare the ROC curve with the ideal and worst classifier\n",
    "- Compute the corresponding AUC using a trapezoidal approximation\n",
    "    - *Example:* Given a curve composed by three points ($x_1,y_1$),($x_2,y_2$),($x_3,y_3$), the AUC is computed as the sum of the trapezoid defined by the points 1-2 and the trapezoid defined by the points 2-3:    $AUC = AUC_{12} + AUC_{23} = \\frac{(x_2-x_1)*(y_1+y_2)}{2} + \\frac{(x_3-x_2)*(y_2+y_3)}{2}$  \n",
    "\n",
    "Based on: \n",
    "- https://mmuratarat.github.io/2019-10-01/how-to-compute-AUC-plot-ROC-by-hand\n",
    "- https://stackoverflow.com/questions/4954507/calculate-the-area-under-a-curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds <- seq(0,0.99,0.05)\n",
    "FPR <- c()\n",
    "TPR <- c()\n",
    "\n",
    "for(threshold in thresholds){\n",
    "   Y_hat <- ifelse(Y_pred[,2] > threshold,\"spam\",\"nonspam\") \n",
    "   confusion_matrix <- table(Y_hat,Y)\n",
    "    \n",
    "   if(dim(confusion_matrix)[1] < 2){ \n",
    "       if(rownames(confusion_matrix) == \"nonspam\"){\n",
    "           confusion_matrix <- rbind(confusion_matrix,c(0,0))\n",
    "           rownames(confusion_matrix)[2] <- \"spam\"\n",
    "       }\n",
    "       if(rownames(confusion_matrix) == \"spam\"){\n",
    "           confusion_matrix <- rbind(c(0,0),confusion_matrix)\n",
    "           rownames(confusion_matrix)[1] <- \"nonspam\"\n",
    "       }\n",
    "   }\n",
    "    \n",
    "   FP <- confusion_matrix[2,1]\n",
    "   TP <- confusion_matrix[2,2]\n",
    "   N_N <- sum(confusion_matrix[,1]) # Total number of nonspam\n",
    "   N_P <- sum(confusion_matrix[,2]) # Total number of spam\n",
    "\n",
    "   FPR <- c(FPR,FP/N_N)\n",
    "   TPR <- c(TPR,TP/N_P)\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAAAAAP9NTU1oaGh8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\nIICQIICQIICQIICQIICQIICQIICQIMD/Y9+bLLg5nVAAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"ROC Curve\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(FPR,TPR)\n",
    "lines(FPR,TPR,col=\"blue\")\n",
    "lines(thresholds,thresholds,lty=2)\n",
    "title(\"ROC Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: pracma\n",
      "\n",
      "Warning message:\n",
      "\"package 'pracma' was built under R version 4.0.4\"\n",
      "\n",
      "Attaching package: 'pracma'\n",
      "\n",
      "\n",
      "The following objects are masked from 'package:kernlab':\n",
      "\n",
      "    cross, eig, size\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "-0.878625408496732"
      ],
      "text/latex": [
       "-0.878625408496732"
      ],
      "text/markdown": [
       "-0.878625408496732"
      ],
      "text/plain": [
       "[1] -0.8786254"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.878625408496732"
      ],
      "text/latex": [
       "0.878625408496732"
      ],
      "text/markdown": [
       "0.878625408496732"
      ],
      "text/plain": [
       "[1] 0.8786254"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "require(pracma)\n",
    "AUC <- trapz(FPR,TPR)\n",
    "AUC\n",
    "AUC <- sum(abs(diff(FPR)) * (head(TPR,-1)+tail(TPR,-1)))/2\n",
    "AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Ensemble learning : random forest\n",
    "\n",
    "Random Forests (RF) is an ensemble learning technique proposed by Breiman which combines bagging and random feature selection by using a large number of non pruned decision trees.\n",
    "Hence, Each individual tree is trained on a different subset of samples (due to bagging), as well as a different subset of features (due to random feature selection)\n",
    "The random feature selection for every tree allows to decorrelate the predictions of the different trees. \n",
    "The aggregation of decorrelated classifiers allows to reduce the variance of the final prediction.\n",
    "In the case of classification, the aggregation is performed by majority vote (i.e. the class that is predicted by the largest number of individual classifiers is selected as the prediction).\n",
    "In the case of regression, the aggregation is performed by performing the average of the individual predictions.\n",
    "\n",
    "For this exercise you need to:\n",
    "- Implement a random forest with five trees from your (simple data partition) DT. Do not use the *randomForest* package.\n",
    "- Test the difference in accuracy with 8 and 20 features per tree.\n",
    "- Implement a 10-fold cross-validation for assessing your RF. Plot the performances according to the number of trained trees. \n",
    "- Bonus: Repeat the previous steps using the  *randomForest* package.\n",
    "\n",
    "Here is an implementation helper:\n",
    "\n",
    "* The number of trees is a compromise between training time and performance.\n",
    "* Each tree is built on a subset of features, a good heuristic being the squared root of the total number of features, drawn randomly.\n",
    "* For a new sample, the predicted class is simply the mean of predicted probabilities on all trained trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         Y\n",
       "Y_hat     nonspam spam\n",
       "  nonspam    1333  155\n",
       "  spam         63  750"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.0947414167753151"
      ],
      "text/latex": [
       "0.0947414167753151"
      ],
      "text/markdown": [
       "0.0947414167753151"
      ],
      "text/plain": [
       "[1] 0.09474142"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_trees <- 5 \n",
    "features_per_tree <- 20\n",
    "threshold <- 0.5\n",
    "\n",
    "spam_idx <- sample(1:nrow(spam))\n",
    "half_split <- floor(nrow(spam)/2)\n",
    "target_variable <- ncol(spam)\n",
    "\n",
    "Y_trees <- numeric()\n",
    "\n",
    "for(i in 1:n_trees){\n",
    "    #3.1 Sample the features\n",
    "    selected_features <- sample(1:(ncol(spam)-1),features_per_tree)\n",
    "    \n",
    "    #3.2 Take the group as a hold out or train data set with bootstrap for each tree\n",
    "    bootstrap_idx <- sample(1:half_split,replace = T)\n",
    "    train_data <- spam[spam_idx[bootstrap_idx],c(selected_features,target_variable)]\n",
    "    \n",
    "    #3.3 Take the remaining groups as a test data set\n",
    "    test_data <- spam[spam_idx[(half_split+1):nrow(spam)],c(selected_features,target_variable)]\n",
    "    print(paste(\"[INFO] - Training set size:\",dim(train_data)[1],\"- Testing set size\",dim(test_data)[1]))\n",
    "    \n",
    "    #3.4 Fit a model on the training set and evaluate it on the test set\n",
    "    model <- rpart(type ~ ., method=\"class\",data=train_data)\n",
    "    Y_pred <- predict(model,subset(test_data, select=-c(type)))\n",
    "    \n",
    "    #3.5 Store the prediction of each tree (2 is to take only the P(Y=\"spam\"|x))\n",
    "    Y_trees <- cbind(Y_trees,Y_pred[,2])\n",
    "}\n",
    "\n",
    "# Calculate the ensemble prediction\n",
    "Y_hat <- apply(Y_trees,1,mean)\n",
    "Y_hat <- ifelse(Y_hat > threshold,\"spam\",\"nonspam\") \n",
    "\n",
    "# Evaluate the predictions\n",
    "Y <- test_data[,\"type\"]\n",
    "confusion_matrix <- table(Y_hat,Y)\n",
    "confusion_matrix\n",
    "\n",
    "accuracy = (confusion_matrix[1,1]+confusion_matrix[2,2])/sum(confusion_matrix)\n",
    "misclassification_rate = 1 - accuracy\n",
    "misclassification_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] - Misclassification rate - 5 features : 0.135593220338983\"\n",
      "[1] \"[INFO] - Misclassification rate - 10 features : 0.13515862668405\"\n",
      "[1] \"[INFO] - Misclassification rate - 15 features : 0.143415906127771\"\n",
      "[1] \"[INFO] - Misclassification rate - 20 features : 0.0838765754019991\"\n",
      "[1] \"[INFO] - Misclassification rate - 25 features : 0.0877879182963929\"\n",
      "[1] \"[INFO] - Misclassification rate - 30 features : 0.0912646675358539\"\n",
      "[1] \"[INFO] - Misclassification rate - 35 features : 0.0951760104302477\"\n",
      "[1] \"[INFO] - Misclassification rate - 40 features : 0.101694915254237\"\n",
      "[1] \"[INFO] - Misclassification rate - 45 features : 0.096045197740113\"\n",
      "[1] \"[INFO] - Misclassification rate - 50 features : 0.0964797913950456\"\n",
      "[1] \"[INFO] - Misclassification rate - 55 features : 0.0977835723598436\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nkFxqxIMAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_trees <- 5 \n",
    "threshold <- 0.5\n",
    "\n",
    "spam_idx <- sample(1:nrow(spam))\n",
    "half_split <- floor(nrow(spam)/2)\n",
    "target_variable <- ncol(spam)\n",
    "\n",
    "feature_values <- seq(5,58,5)\n",
    "accuracy_vec <- numeric()\n",
    "\n",
    "for (features_per_tree in feature_values)\n",
    "{\n",
    "    Y_trees <- numeric()\n",
    "\n",
    "    for(i in 1:n_trees){\n",
    "        #3.1 Sample the features\n",
    "        selected_features <- sample(1:(ncol(spam)-1),features_per_tree)\n",
    "        \n",
    "        #3.2 Take the first half of the dataset as a training data set with bootstrap for each tree\n",
    "        bootstrap_idx <- sample(1:half_split,replace = T)\n",
    "        train_data <- spam[spam_idx[bootstrap_idx],c(selected_features,target_variable)]\n",
    "\n",
    "        #3.3 Take the second half of the dataset as a hold out or test data set\n",
    "        test_data <- spam[spam_idx[(half_split+1):nrow(spam)],c(selected_features,target_variable)]\n",
    "        #print(paste(\"[INFO] - Training set size:\",dim(train_data)[1],\"- Testing set size\",dim(test_data)[1]))\n",
    "\n",
    "        #3.4 Fit a model on the training set and evaluate it on the test set\n",
    "        model <- rpart(type ~ ., method=\"class\",data=train_data)\n",
    "        Y_pred <- predict(model,subset(test_data, select=-c(type)))\n",
    "\n",
    "        #3.5 Store the prediction of each tree (2 is to take only the P(Y=\"spam\"|x))\n",
    "        Y_trees <- cbind(Y_trees,Y_pred[,2])\n",
    "    }\n",
    "\n",
    "    # Calculate the ensemble prediction\n",
    "    Y_hat <- apply(Y_trees,1,mean)\n",
    "    Y_hat <- ifelse(Y_hat > threshold,\"spam\",\"nonspam\") \n",
    "\n",
    "    # Evaluate the predictions\n",
    "    Y <- test_data[,\"type\"]\n",
    "    confusion_matrix <- table(Y_hat,Y)\n",
    "    confusion_matrix\n",
    "\n",
    "    accuracy = (confusion_matrix[1,1]+confusion_matrix[2,2])/sum(confusion_matrix)\n",
    "    misclassification_rate = 1 - accuracy\n",
    "    accuracy_vec <- cbind(accuracy_vec,accuracy)\n",
    "    print(paste(\"[INFO] - Misclassification rate -\",features_per_tree,\"features :\",misclassification_rate))\n",
    "}\n",
    "\n",
    "plot(feature_values,accuracy_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] - Misclassification rate - 1 fold: 0.0607375271149675\"\n",
      "[1] \"[INFO] - Misclassification rate - 2 fold: 0.0976138828633406\"\n",
      "[1] \"[INFO] - Misclassification rate - 3 fold: 0.0845986984815619\"\n",
      "[1] \"[INFO] - Misclassification rate - 4 fold: 0.106290672451193\"\n",
      "[1] \"[INFO] - Misclassification rate - 5 fold: 0.0911062906724512\"\n",
      "[1] \"[INFO] - Misclassification rate - 6 fold: 0.0954446854663774\"\n",
      "[1] \"[INFO] - Misclassification rate - 7 fold: 0.112798264642082\"\n",
      "[1] \"[INFO] - Misclassification rate - 8 fold: 0.0694143167028199\"\n",
      "[1] \"[INFO] - Misclassification rate - 9 fold: 0.0997830802603037\"\n",
      "[1] \"[INFO] - Misclassification rate - 10 fold: 0.0929203539823009\"\n",
      "[1] \"[INFO] - Mean misclassification rate: 0.0910707772637399\"\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "n_trees <- 5 \n",
    "features_per_tree <- 20\n",
    "\n",
    "target_variable <- ncol(spam)\n",
    "accuracy_vec <- array(0,k)\n",
    "threshold <- 0.5\n",
    "\n",
    "# 1. Shuffle the dataset randomly.\n",
    "spam_idx <- sample(1:nrow(spam))\n",
    "\n",
    "# 2. Split the dataset into k groups\n",
    "max <- ceiling(nrow(spam)/k)\n",
    "splits <- split(spam_idx, ceiling(seq_along(spam_idx)/max))\n",
    "\n",
    "# 3. For each unique group:\n",
    "for (i in 1:k){\n",
    "    \n",
    "    Y_trees <- numeric()\n",
    "    \n",
    "    for(j in 1:n_trees){\n",
    "        #3.1 Select features for the tree\n",
    "        selected_features <- sample(1:(ncol(spam)-1),features_per_tree)\n",
    "        \n",
    "        #3.2 Take the group as a hold out or test data set\n",
    "        test_data <- spam[splits[[i]],c(selected_features,target_variable)]\n",
    "\n",
    "        #3.3 Take the remaining groups as a training data set with bootstrap for each tree\n",
    "        bootstrap_idx <- sample(-splits[[i]],replace = T)\n",
    "        train_data <- spam[bootstrap_idx,c(selected_features,target_variable)]\n",
    "        #print(paste(\"[INFO] - Training set size:\",dim(train_data)[1],\"- Testing set size\",dim(test_data)[1]))\n",
    "\n",
    "        #3.4 Fit a model on the training set and evaluate it on the test set\n",
    "        model <- rpart(type ~ ., method=\"class\",data=train_data)\n",
    "        Y_pred <- predict(model,subset(test_data, select=-c(type)))\n",
    "\n",
    "        # Store the prediction of each tree\n",
    "        Y_trees <- cbind(Y_trees,Y_pred[,2])\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Calculate the ensemble prediction\n",
    "    Y_hat <- apply(Y_trees,1,mean)\n",
    "    Y_hat <- ifelse(Y_hat > threshold,\"spam\",\"nonspam\") \n",
    "\n",
    "    # Evaluate the predictions\n",
    "    Y <- test_data[,\"type\"]\n",
    "    confusion_matrix <- table(Y_hat,Y)\n",
    "    confusion_matrix\n",
    "    \n",
    "    #3.4 Retain the evaluation score and discard the model\n",
    "    accuracy_vec[i] = (confusion_matrix[1,1]+confusion_matrix[2,2])/sum(confusion_matrix)\n",
    "    misclassification_rate = 1 - accuracy_vec[i]\n",
    "    print(paste(\"[INFO] - Misclassification rate -\",i,\"fold:\",misclassification_rate))\n",
    "}\n",
    "\n",
    "#4. Summarize the accuracy of the model using the sample of model evaluation scores\n",
    "print(paste(\"[INFO] - Mean misclassification rate:\",1-mean(accuracy_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomForest package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomForest 4.6-14\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "\n",
      "Attaching package: 'randomForest'\n",
      "\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    margin\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nYII=",
      "text/plain": [
       "Plot with title \"Number of trees influence\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(\"randomForest\")\n",
    "n_trees <- 20\n",
    "accuracy_vec <- array(0,n_trees)\n",
    "\n",
    "spam_idx <- sample(1:nrow(spam))\n",
    "half_split <- floor(nrow(spam)/2)\n",
    "target_variable <- ncol(spam)\n",
    "\n",
    "for (i in 1:n_trees){ #print(i)\n",
    "    #3.1 Take the first half of the dataset as a training data set\n",
    "    train_data <- spam[spam_idx[1:half_split],]\n",
    "\n",
    "    #3.2 Take the second half of the dataset as a hold out or test data set\n",
    "    test_data <- spam[spam_idx[(half_split+1):nrow(spam)],]\n",
    "    \n",
    "    model <- randomForest(x=train_data[,-c(target_variable)],\n",
    "                          y=as.factor(train_data[,c(target_variable)]),\n",
    "                          xtest=test_data[,-c(target_variable)],\n",
    "                          ytest=as.factor(test_data[,c(target_variable)]),\n",
    "                          ntree=i)\n",
    "    \n",
    "    accuracy_vec[i] = (model$test$confusion[1,1]+model$test$confusion[2,2])/sum(model$test$confusion)\n",
    "    }\n",
    "\n",
    "plot(accuracy_vec,main = \"Number of trees influence\",xlab = \"Nbr of trees\",ylab = \"Classification rate\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Ensemble learning : Boosting (Ada Boost)\n",
    "\n",
    "Boosting is an additional generic ensemble technique which attempts to boost the accuracy of any given learning algorithm.\n",
    "The focus of boosting methods is to produce a series of weak learners in order to produce a powerful combination. \n",
    "A weak learner is a learner that has accuracy only slightly better than chance.\n",
    "In boosting, the training set used for each member of the series is chosen based on the performance of the earlier classifier(s) in the series. \n",
    "Examples that are incorrectly predicted by previous classifiers in the series are chosen more often than examples that were correctly predicted.\n",
    "Thus Boosting attempts to produce new classifiers that are better able to predict examples for which the current ensemble‚Äôs performance is poor. \n",
    "Unlike Bagging (employed in Random Forests), the resampling of the training set is dependent on the performance of the earlier classifiers, which prevents a parallel implementation of the ensembling procedure.\n",
    "\n",
    "For this exercise you need to:\n",
    "- Implement a boosting ensemble of 15 decision trees using Ada Boost (cf. 11.3.1)\n",
    "- The initial sampling probability for each sample $i$ is $w_i = \\frac{1}{N}$ (with $1 \\leq i \\leq N$, $N$ being the number of samples in train set). \n",
    "- After the first tree, $w_i$ is adjusted according to the classification performances:\n",
    "    \\begin{equation} w_i = w_i \\begin{cases} e^{-\\alpha_j} & \\text{if sample $i$ correctly classified} \\\\ e^{\\alpha_j} & \\text{if sample $i$ incorrectly classified} \\end{cases} \\end{equation}\n",
    "    \\begin{equation} \\alpha_j = \\log(\\frac{1-ER_j}{ER_j}) \\end{equation}\n",
    "    \\begin{equation} ER_j = \\frac{\\sum_{i=1}^{N} w_{i} I\\left(y_{i} \\neq h_{j}\\left(x_{i}\\right)\\right)}{\\sum_{i=1}^{N} w_{i}} \\end{equation}\n",
    "      \n",
    "- The prediction of the boosting ensemble is defined as a linear combination (weighted by $\\alpha_j$) of the outputs of the different trees:\n",
    "    \\begin{equation} h_{boo} = sign\\left( \\sum_{j=1}^{m} = \\alpha_j*h_j(x)\\right) \\end{equation}\n",
    "\n",
    "    \n",
    "\n",
    "Here is an implementation helper (multiple variants exist) :\n",
    "\n",
    "- The number of trees is a compromise between training time and performance. \n",
    "- The trees are usually depth-forced : add *control=tree.control(Ntrain,mincut=10)* as a parameter to *tree()*.\n",
    "- For this exercice, we will use the *tree* package. It is therefore easier to recode the spam labels as -1/+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsample <- nrow(spam)\n",
    "data(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare, in terms of misclassifcation, a regular tree against a boosted 15-trees model. You can re-use the code of the ''very simple partition'' as for the first tree we designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in Y_pred_boosting + alpha_vec[j] * predict(tree_model, test_data):\n",
      "\"longer object length is not a multiple of shorter object length\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Training set size: 2300 - Testing set size 2301\"\n",
      "[1] \"[INFO] - Misclassification rate - Single tree: 0.0999565406345067\"\n",
      "[1] \"[INFO] - Misclassification rate - Boosted tree: 0.050412863972186\"\n"
     ]
    }
   ],
   "source": [
    "library(tree)\n",
    "\n",
    "# Parameters of the algorithm\n",
    "n_trees <- 15\n",
    "N <- nrow(spam)\n",
    "\n",
    "alpha_vec <- array(NA,n_trees)\n",
    "misclassification_vec <- array(NA,n_trees)\n",
    "\n",
    "# Rescaling target variable\n",
    "data(spam)\n",
    "spam$type <- ifelse(spam$type == \"nonspam\",-1,1) # Data in {-1,+1}\n",
    "target_variable <- ncol(spam)\n",
    "\n",
    "\n",
    "# 50/50 data split\n",
    "spam_idx <- sample(1:N)\n",
    "half_split <- floor(N/2)\n",
    "train_data <- spam[spam_idx[1:half_split],]\n",
    "test_data <- spam[spam_idx[(half_split+1):nrow(spam)],]\n",
    "\n",
    "\n",
    "# Prediction - Single tree\n",
    "tree_model <-tree(type ~ ., train_data,\n",
    "                  control=tree.control(half_split,mincut=10))\n",
    "Y_pred_tree <- sign(predict(tree_model,test_data))\n",
    "\n",
    "# Boosting\n",
    "Y_pred_boosting <- rep(0,length(test_data))\n",
    "w <- array(1/half_split,half_split)\n",
    "\n",
    "set.seed(555)\n",
    "\n",
    "# For each boosting iteration\n",
    "for(j in 1:n_trees){\n",
    "    \n",
    "    #1. Sample from the training set\n",
    "    selected_samples <- sample(1:half_split,prob=w,replace=TRUE)\n",
    "    \n",
    "    print(paste(\"[INFO] - Training set size:\",dim(train_data)[1],\"- Testing set size\",dim(test_data)[1]))\n",
    "    \n",
    "    #2. Fit a model on the training set and evaluate it on the training set\n",
    "    tree_model <-tree(type ~ ., train_data[selected_samples,],\n",
    "                  control=tree.control(half_split,mincut=10))\n",
    "    Y_pred_train <- sign(predict(tree_model,train_data))\n",
    "    \n",
    "    #3. Compute misclassification on training set\n",
    "    misclassification_vec[j] <- sum(w*as.integer(train_data$type != sign(Y_pred_train)))/sum(w)\n",
    "    \n",
    "    #4. Computation of alpha and updating of resampling weight\n",
    "    alpha_vec[j] <- log((1-misclassification_vec[j])/misclassification_vec[j])\n",
    "    w <- w * exp(alpha_vec[j]*as.integer(train_data$type != sign(Y_pred_train)))\n",
    "    \n",
    "    #5. Normalize the weights in order to represent a true distribution\n",
    "    w <- w/sum(w)\n",
    "    \n",
    "    #6. Compute the predictions on the testing set and weight them by alpha\n",
    "    Y_pred_boosting<-Y_pred_boosting+alpha_vec[j]*predict(tree_model,test_data)\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# Misclassification error comparation\n",
    "misclassification_error_base <- sum(as.integer(test_data$type != sign(Y_pred_tree)))/length(Y_pred_tree)\n",
    "misclassification_error_boosting <- sum(as.integer(test_data$type != sign(Y_pred_boosting)))/length(Y_pred_boosting)\n",
    "\n",
    "print(paste(\"[INFO] - Misclassification rate - Single tree:\",misclassification_error_base))\n",
    "print(paste(\"[INFO] - Misclassification rate - Boosted tree:\",misclassification_error_boosting))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
